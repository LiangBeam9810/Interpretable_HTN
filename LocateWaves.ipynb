{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ECGDataset \n",
    "import Models \n",
    "import Net\n",
    "from train_test_validat import *\n",
    "from self_attention import *\n",
    "import matplotlib.pyplot as plt\n",
    "import ecg_plot\n",
    "import cam\n",
    "import ECGplot\n",
    "import ECGHandle\n",
    "import torch\n",
    "import torch.utils.data as Data\n",
    "import sklearn\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import random\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "import time\n",
    "import math\n",
    "import os\n",
    "import gc\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "\n",
    "def seed_torch(seed=2023):\n",
    "\trandom.seed(seed)\n",
    "\tos.environ['PYTHONHASHSEED'] = str(seed) # 为了禁止hash随机化，使得实验可复现\n",
    "\tnp.random.seed(seed)\n",
    "\ttorch.manual_seed(seed)\n",
    "\ttorch.cuda.manual_seed(seed)\n",
    "\ttorch.cuda.manual_seed_all(seed) # if you are using multi-GPU.\n",
    "\ttorch.backends.cudnn.benchmark = False \n",
    "\ttorch.backends.cudnn.deterministic = True\n",
    "    # torch.backends.cudnn.enabled = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "EcgChannles_num = 12\n",
    "EcgLength_num = 5000\n",
    "DEVICE = \"cpu\"\n",
    "\n",
    "# DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "seed_torch(2023)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "            orginal   removed diagnose NaN\n",
      "   nums      200082          199997       \n",
      "              HTN             NHTN        \n",
      "   nums       3273           196724       \n",
      "\n",
      "\n",
      "            orginal      removed ID NaN   \n",
      "   nums      199997          199995       \n",
      "              HTN             NHTN        \n",
      "   nums       3273           196722       \n",
      "\n",
      "\n",
      "            orginal            QC         \n",
      "   nums      199995          72845        \n",
      "              HTN             NHTN        \n",
      "   nums       1497           71348        \n",
      "\n",
      "\n",
      "            orginal      filtered ages    \n",
      "   nums      72845           69819        \n",
      "              HTN             NHTN        \n",
      "   nums       1477           68342        \n",
      "\n",
      "\n",
      "     reset num:       10  \n",
      "  ERR labels num:     308 \n",
      "            orginal      correct label    \n",
      "   nums      69819           69819        \n",
      "              HTN             NHTN        \n",
      "   nums       1794           68025        \n",
      "\n",
      "\n",
      "            orginal   filtered department \n",
      "   nums      69819           15625        \n",
      "              HTN             NHTN        \n",
      "   nums       1794           13831        \n",
      "\n",
      "\n",
      "   ERR ages num:     1486 \n",
      "            orginal       correct age     \n",
      "   nums      15625           15625        \n",
      "              HTN             NHTN        \n",
      "   nums       1794           13831        \n",
      "\n",
      "\n",
      "            orginal    remove diagnose起搏  \n",
      "   nums      15625           15552        \n",
      "              HTN             NHTN        \n",
      "   nums       1768           13784        \n",
      "           remove HTN     remove NHTN     \n",
      "   nums        26              47         \n",
      "\n",
      "\n",
      "            orginal    remove diagnose房颤  \n",
      "   nums      15552           14966        \n",
      "              HTN             NHTN        \n",
      "   nums       1651           13315        \n",
      "           remove HTN     remove NHTN     \n",
      "   nums       117             469         \n",
      "\n",
      "\n",
      "            orginal   remove diagnose左束支传导阻滞\n",
      "   nums      14966           14966        \n",
      "              HTN             NHTN        \n",
      "   nums       1651           13315        \n",
      "           remove HTN     remove NHTN     \n",
      "   nums        0               0          \n",
      "\n",
      "\n",
      "            orginal   remove diagnose左前分支阻滞\n",
      "   nums      14966           14811        \n",
      "              HTN             NHTN        \n",
      "   nums       1632           13179        \n",
      "           remove HTN     remove NHTN     \n",
      "   nums        19             136         \n"
     ]
    }
   ],
   "source": [
    "data_root = '/workspace/data/Preprocess_HTN/datas_/'\n",
    "ALL_data = pd.read_csv(data_root+'/All_data_handled_ID_range_age_IDimputate.csv',low_memory=False)\n",
    "ALL_data = ECGHandle.change_label(ALL_data)\n",
    "ALL_data = ECGHandle.filter_ID(ALL_data)\n",
    "ALL_data = ECGHandle.filter_QC(ALL_data)\n",
    "ALL_data = ECGHandle.filter_ages(ALL_data,18)\n",
    "ALL_data = ECGHandle.filter_departmentORlabel(ALL_data,'外科')\n",
    "ALL_data = ECGHandle.correct_label(ALL_data)\n",
    "ALL_data = ECGHandle.correct_age(ALL_data)\n",
    "ALL_data = ECGHandle.filter_diagnose(ALL_data,'起搏')\n",
    "ALL_data = ECGHandle.filter_diagnose(ALL_data,'房颤')\n",
    "ALL_data = ECGHandle.filter_diagnose(ALL_data,'左束支传导阻滞')\n",
    "ALL_data = ECGHandle.filter_diagnose(ALL_data,'左前分支阻滞')\n",
    "# ALL_data = ECGHandle.filter_diagnose(ALL_data,'阻滞')\n",
    "# ALL_data = ECGHandle.remove_duplicated(ALL_data)\n",
    "ALL_data = ALL_data.rename(columns={'住院号':'ID','年龄':'age','性别':'gender','姓名':'name'}) \n",
    "ALL_data_buffer = ALL_data.copy()\n",
    "seed_torch(2023)\n",
    "ALL_data_buffer = ALL_data_buffer.sample(frac=1).reset_index(drop=True) #打乱顺序\n",
    "# all_dataset = ECGHandle.ECG_Dataset(data_root,ALL_data_buffer,preprocess = True)\n",
    "####################################################################随机选取test\n",
    "test_df,tv_df = Pair_ID(ALL_data,0.2,Range_max=15,pair_num=1)\n",
    "test_dataset = ECGHandle.ECG_Dataset(data_root,test_df,preprocess = True)\n",
    "\n",
    "\n",
    "Models_path = 'model/20230302_080241/20230302_080241/parameter_EarlyStoping_3.pt'\n",
    "save_root = Models_path[:-3]+'/'\n",
    "layervalue_root = save_root+'/layervalue/'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "     reset num:       10  \n",
      "  ERR labels num:     308 \n",
      "            orginal      correct label    \n",
      "   nums      69819           69819        \n",
      "              HTN             NHTN        \n",
      "   nums       1794           68025        \n",
      "\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_3812/953613546.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mALL_data_add\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mECGHandle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorrect_label\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mALL_data_add\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mALL_data_add\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mECGHandle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorrect_age\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mALL_data_add\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mALL_data_add\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mECGHandle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilter_diagnose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mALL_data_add\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'起搏'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mALL_data_add\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mECGHandle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilter_diagnose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mALL_data_add\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'房颤'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mALL_data_add\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mECGHandle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilter_diagnose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mALL_data_add\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'左束支传导阻滞'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/workspace/data/Interpretable_HTN/ECGHandle.py\u001b[0m in \u001b[0;36mcorrect_age\u001b[0;34m(df_input)\u001b[0m\n\u001b[1;32m    192\u001b[0m         \u001b[0mage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'年龄'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    193\u001b[0m         \u001b[0mID\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'住院号'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 194\u001b[0;31m         \u001b[0mdf_buffer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_filter\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_filter\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'住院号'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mID\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m&\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m~\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_filter\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'年龄'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;31m#住院号相同 但年龄不同\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    195\u001b[0m         \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_buffer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mindex_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mrow_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdf_buffer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterrows\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;31m#遍历所有符合条件的样本，赋予相同年龄\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/core/ops/common.py\u001b[0m in \u001b[0;36mnew_method\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[0mother\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mitem_from_zerodim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mnew_method\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/core/arraylike.py\u001b[0m in \u001b[0;36m__eq__\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0munpack_zerodim_and_defer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"__eq__\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__eq__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cmp_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moperator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0munpack_zerodim_and_defer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"__ne__\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/core/series.py\u001b[0m in \u001b[0;36m_cmp_method\u001b[0;34m(self, other, op)\u001b[0m\n\u001b[1;32m   5500\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5501\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merrstate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"ignore\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5502\u001b[0;31m             \u001b[0mres_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcomparison_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5503\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5504\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_construct_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres_values\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mres_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/numpy/core/_ufunc_config.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, *exc_info)\u001b[0m\n\u001b[1;32m    433\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    434\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mexc_info\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 435\u001b[0;31m         \u001b[0mseterr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moldstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    436\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_Unspecified\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    437\u001b[0m             \u001b[0mseterrcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moldcall\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/numpy/core/_ufunc_config.py\u001b[0m in \u001b[0;36mseterr\u001b[0;34m(all, divide, over, under, invalid)\u001b[0m\n\u001b[1;32m    124\u001b[0m                  (_errdict[invalid] << SHIFT_INVALID))\n\u001b[1;32m    125\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 126\u001b[0;31m     \u001b[0mpyvals\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmaskvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    127\u001b[0m     \u001b[0mumath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseterrobj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpyvals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mold\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "data_root = '/workspace/data/Preprocess_HTN/datas_/'\n",
    "ALL_data = pd.read_csv(data_root+'/All_data_handled_ID_range_age_IDimputate.csv',low_memory=False)\n",
    "ALL_data = ECGHandle.change_label(ALL_data)\n",
    "ALL_data = ECGHandle.filter_ID(ALL_data)\n",
    "ALL_data = ECGHandle.filter_QC(ALL_data)\n",
    "ALL_data = ECGHandle.filter_ages(ALL_data,18)\n",
    "ALL_data = ECGHandle.filter_departmentORlabel(ALL_data,'外科')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义获取梯度的函数\n",
    "fmap_block = list()\n",
    "grad_block = list()\n",
    "\n",
    "# 获取反向的传播图\n",
    "def backward_hook(module, grad_in, grad_out):\n",
    "    # print('backward_hook ',len(grad_out),grad_out[0].shape)\n",
    "    grad_block.append(grad_out[0].clone().cpu().detach())\n",
    "\n",
    "# 定义获取特征图的函数\n",
    "def farward_hook(module, input, output):\n",
    "    # print('farward_hook ',output.shape)\n",
    "    fmap_block.append(output.clone().cpu().detach())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_layer_cam(x,gamma:float = 2.):\n",
    "    x_max = x.max()\n",
    "    x_scaled = np.tanh(((gamma*x)/x_max))\n",
    "    return x_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_layer_cam_max(cam):\n",
    "    (cam_min, cam_max) = (cam.min(), cam.max())\n",
    "    norm_cam = (cam - cam_min) / (((cam_max - cam_min) + 1e-08))\n",
    "    return norm_cam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Models_path = '/workspace/data/Interpretable_HTN/model/20230307_144157/20230307_144157/BestF1_0.pt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NET = [Net.MLBFNet_GUR_o(True,True,True,2,Dropout_rate=0.3), ] # type: ignore\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "testmodel = NET[0].to(DEVICE)\n",
    "testmodel.load_state_dict(torch.load(Models_path))\n",
    "test_dataloader = Data.DataLoader(dataset=test_dataset, batch_size=1, shuffle=False)  \n",
    "y_true,y_pred,y_out,test_loss,test_acc = eval_model(test_dataloader,criterion,testmodel,DEVICE) # 验证模型"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 计算出最佳阈值，并获得该阈值下的y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Find_Optimal_Cutoff(TPR, FPR, threshold):\n",
    "    y = TPR - FPR\n",
    "    Youden_index = np.argmax(y)  # Only the first occurrence is returned.\n",
    "    optimal_threshold = threshold[Youden_index]\n",
    "    point = [FPR[Youden_index], TPR[Youden_index]]\n",
    "    return optimal_threshold, point\n",
    "def ROC(label, y_prob):\n",
    "    \"\"\"\n",
    "    Receiver_Operating_Characteristic, ROC\n",
    "    :param label: (n, )\n",
    "    :param y_prob: (n, )\n",
    "    :return: fpr, tpr, roc_auc, optimal_th, optimal_point\n",
    "    \"\"\"\n",
    "    fpr, tpr, thresholds = sklearn.metrics.roc_curve(label, y_prob)\n",
    "    roc_auc = sklearn.metrics.auc(fpr, tpr)\n",
    "    optimal_th, optimal_point = Find_Optimal_Cutoff(TPR=tpr, FPR=fpr, threshold=thresholds)\n",
    "    return fpr, tpr, roc_auc, optimal_th, optimal_point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr, tpr, roc_auc, optimal_th, optimal_point = ROC(y_true,((np.array(y_out))[:,1]))\n",
    "plt.figure(1)\n",
    "plt.plot(fpr, tpr, label=f\"AUC = {roc_auc:.3f}\")\n",
    "plt.plot([0, 1], [0, 1], linestyle=\"--\")\n",
    "plt.plot(optimal_point[0], optimal_point[1], marker='o', color='r')\n",
    "plt.text(optimal_point[0], optimal_point[1], f'Threshold:{optimal_th:.2f}')\n",
    "plt.title(\"ROC-AUC\")\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "CM = confusion_matrix(y_true,y_pred)\n",
    "print(CM)\n",
    "y_pred = ((np.array(y_out))[:,1])\n",
    "y_pred[y_pred>optimal_th] =  1\n",
    "y_pred[y_pred<=optimal_th] =  0"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 计算layer-CAM值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_root = Models_path[:-3]+'/'\n",
    "layervalue_root = save_root+'/layervalue/'\n",
    "if(not(os.path.exists(save_root))): os.mkdir(save_root)\n",
    "if(not(os.path.exists(layervalue_root))): os.mkdir(layervalue_root)\n",
    "lead_index = ['I', 'II', 'III', 'aVR', 'aVL', 'aVF', 'V1', 'V2', 'V3', 'V4', 'V5', 'V6']\n",
    "x_index = np.arange(0,12)\n",
    "# fig, axs = plt.subplots(nrows=4, ncols=3, sharex=True,sharey=True,figsize=(45,25), constrained_layout=True,dpi =100)\n",
    "for index in tqdm(range( test_dataset.__len__())):# test_dataset.__len__()\n",
    "    NET = [Net.MLBFNet_GUR_o(True,True,True,2,Dropout_rate=0.3), ] # type: ignore\n",
    "    testmodel = NET[0].to(DEVICE)\n",
    "    testmodel.load_state_dict(torch.load(Models_path))\n",
    "    testmodel.conv1.register_forward_hook(farward_hook)\t#正向传播\n",
    "    testmodel.conv1.register_full_backward_hook(backward_hook)#反向传播\n",
    "    testmodel.conv2.register_forward_hook(farward_hook)\t#正向传播\n",
    "    testmodel.conv2.register_full_backward_hook(backward_hook)#反向传播\n",
    "    testmodel.layers_list_2d[0].register_forward_hook(farward_hook)\t#正向传播\n",
    "    testmodel.layers_list_2d[0].register_full_backward_hook(backward_hook)#反向传播\n",
    "    testmodel.layers_list_2d[1].register_forward_hook(farward_hook)\t#正向传播\n",
    "    testmodel.layers_list_2d[1].register_full_backward_hook(backward_hook)#反向传播\n",
    "    testmodel.layers_list_2d[2].register_forward_hook(farward_hook)\t#正向传播\n",
    "    testmodel.layers_list_2d[2].register_full_backward_hook(backward_hook)#反向传播\n",
    "\n",
    "\n",
    "    inputs,labels = test_dataset.__getitem__(index)\n",
    "    info =test_dataset.infos.iloc[index]\n",
    "    ECGfile_name = info['ECGFilename']\n",
    "    testmodel.eval()\n",
    "    fmap_block = list()\n",
    "    grad_block = list()\n",
    "\n",
    "    labels = labels.unsqueeze(0) # 在首位添加1维作为batchsize\n",
    "    inputs = inputs.unsqueeze(0) # 在首位添加1维作为batchsize\n",
    "\n",
    "    inputs = inputs.to(DEVICE)\n",
    "    labels = labels.to(DEVICE)  \n",
    "\n",
    "    outputs = testmodel(inputs)\n",
    "    # print('outputs',outputs)\n",
    "    _,pred = outputs.max(1)     # 求概率最大值对应的标签\n",
    "    \n",
    "    possibility = nn.functional.softmax(outputs,dim=-1)\n",
    "    # print(\"labels: {}\".format(labels))\n",
    "    # print(\"predict: {}\".format(pred))\n",
    "    # print(\"possibility: {}\".format(possibility))\n",
    "    loss = outputs[0,int(y_pred[index])]      # 网络对应于pred的类别的输出即为loss\n",
    "    loss.backward(retain_graph=True)  #retain_graph=True，目的为是为保留该过程中计算的梯度，后续G网络更新时使用\n",
    "\n",
    "    fmap1 = (fmap_block[0][0]).to('cpu').detach().numpy()\n",
    "    gradmap1 = (grad_block[4][0]).to('cpu').detach().numpy()#G是从后往前的，conv1的在后面\n",
    "    layer2d_vlue1 = cam.caculate_layer_cam_vlue_2d(fmap1,gradmap1)\n",
    "\n",
    "    fmap2 = (fmap_block[1][0]).to('cpu').detach().numpy()\n",
    "    gradmap2 = (grad_block[3][0]).to('cpu').detach().numpy()\n",
    "    layer2d_vlue2 = cam.caculate_layer_cam_vlue_2d(fmap2,gradmap2)\n",
    "\n",
    "    fmap2d_1 = (fmap_block[2][0]).to('cpu').detach().numpy()\n",
    "    gradmap2d_1 = (grad_block[2][0]).to('cpu').detach().numpy()\n",
    "    layer2d_vlue2d_1 = cam.caculate_layer_cam_vlue_2d(fmap2d_1,gradmap2d_1)\n",
    "\n",
    "    fmap2d_2 = (fmap_block[3][0]).to('cpu').detach().numpy()\n",
    "    gradmap2d_2 = (grad_block[1][0]).to('cpu').detach().numpy()\n",
    "    layer2d_vlue2d_2 = cam.caculate_layer_cam_vlue_2d(fmap2d_2,gradmap2d_2)\n",
    "\n",
    "    fmap2d_3 = (fmap_block[4][0]).to('cpu').detach().numpy()\n",
    "    gradmap2d_3 = (grad_block[0][0]).to('cpu').detach().numpy()\n",
    "    layer2d_vlue2d_3 = cam.caculate_layer_cam_vlue_2d(fmap2d_3,gradmap2d_3)\n",
    "\n",
    "    layer_vlue_mean = (scaled_layer_cam_max(layer2d_vlue1)+scaled_layer_cam_max(layer2d_vlue2)+scaled_layer_cam_max(layer2d_vlue2d_1)+scaled_layer_cam_max(layer2d_vlue2d_2)+scaled_layer_cam_max(layer2d_vlue2d_3))/5\n",
    "    np.save(layervalue_root+ECGfile_name,layer_vlue_mean)\n",
    "    \n",
    "#     inputs,_ = test_dataset.__getitem__(index)\n",
    "#     cmap = 'OrRd' \n",
    "#     colors = ECGplot.color_map(layer_vlue_mean, cmap)#相当于归一化\n",
    "\n",
    "#     for i,ax in enumerate(axs.flat):  # type: ignore\n",
    "#         plot_y = np.array(inputs[i]*5000.)#缩放\n",
    "#         t = np.arange(0,5000)\n",
    "#         ECGplot.plot_ECG_line(fig,ax,x = t,y= plot_y,y_name = str(lead_index[i])+\" Voltage(mV)\",title = lead_index[i])\n",
    "#         for j in t:\n",
    "#             ax.axvline(x=j,alpha=0.1,color=colors[i,j]) #第i导联 第j个时间点的importance value\n",
    "#     plt.savefig(save_root+'/'+info['ECGFilename']+'_'+str(labels[0].tolist())+'_'+str(pred[0].tolist())+'.png', bbox_inches='tight',dpi = 200)  \n",
    "# plt.close()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_min(x):\n",
    "    max_ = x.max()\n",
    "    min_ = x.min()\n",
    "    x_ = (x-min_)/(max_-min_+1e-8)\n",
    "    return x_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def layervalue_segement_count(start_index:int,end_index:int,layervalue:np.ndarray,segement_num:int = 10):\n",
    "    len_ = end_index-start_index\n",
    "    chanle_num = layervalue.shape[0]\n",
    "    if(len_<segement_num):\n",
    "        layervalue_sum = ((layervalue[:,start_index:end_index].sum())/(chanle_num*segement_num*1.0))\n",
    "        return layervalue_sum\n",
    "    step = len_//segement_num\n",
    "    layervalue_sum = np.zeros(segement_num)\n",
    "    for i in range(segement_num):\n",
    "        start = start_index+step*i\n",
    "        sum_sement = layervalue[:,start:(start+step)].sum()/(chanle_num*step*1.0)\n",
    "        layervalue_sum[i] = layervalue_sum[i]+sum_sement*1. #在这一段step里面，每个时间点的平均重要程度\n",
    "    return layervalue_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 0\n",
    "segement_number = 10 # 共6个片段，每段划分成10份\n",
    "important_values = np.zeros([6,segement_number])\n",
    "\n",
    "\n",
    "for index in range(test_dataset.__len__()):#\n",
    "    important_value = np.zeros([6,segement_number])\n",
    "    inputs,labels = test_dataset.__getitem__(index)\n",
    "    if(labels != y_true[index]): \n",
    "        break\n",
    "    pred = y_pred[index]\n",
    "    if((pred == 0) or (labels == 0)): continue;\n",
    "    info =test_dataset.infos.iloc[index]\n",
    "    ECGfile_name = info['ECGFilename']\n",
    "    layervalue_file = layervalue_root  + ECGfile_name +'.npy'\n",
    "    waves_location_file_root = '/workspace/data/Preprocess_HTN/Waves_2023/'\n",
    "    waves = pd.read_csv(waves_location_file_root+ECGfile_name+'.csv',index_col=0)\n",
    "    if(waves.isnull().any().any()):\n",
    "        continue\n",
    "    layervalue = np.load(layervalue_file)\n",
    "\n",
    "    for head_peak in range(1,len(waves.columns)-1):\n",
    "        if(waves.loc['P'][head_peak] == waves.iloc[:,head_peak].min()): #P波不是第一个识别出来的波形\n",
    "            p_index = int(waves.loc['P'][head_peak])\n",
    "            q_index = int(waves.loc['Q'][head_peak])\n",
    "            r_index = int(waves.loc['R'][head_peak])\n",
    "            s_index = int(waves.loc['S'][head_peak])\n",
    "            t_index = int(waves.loc['T'][head_peak])\n",
    "            t_index_befort = int(waves.loc['T'][head_peak-1])\n",
    "            if(((waves.loc['P'][head_peak+1]==None)) or((waves.loc['P'][head_peak+1] < t_index))):\n",
    "                break\n",
    "            else:\n",
    "                p_index_next = waves.loc['P'][head_peak+1] \n",
    "            important_value[0] = important_value[0]+ layervalue_segement_count(t_index_befort,p_index,layervalue,segement_number)\n",
    "            important_value[1] = important_value[1]+ layervalue_segement_count(p_index,q_index,layervalue,segement_number)   \n",
    "            important_value[2] = important_value[2]+ layervalue_segement_count(q_index,r_index,layervalue,segement_number) \n",
    "            important_value[3] = important_value[3]+layervalue_segement_count(r_index,s_index,layervalue,segement_number)\n",
    "            important_value[4] = important_value[4]+ layervalue_segement_count(s_index,t_index,layervalue,segement_number) \n",
    "            important_value[5] = important_value[5]+ layervalue_segement_count(t_index,p_index_next,layervalue,segement_number) \n",
    "        else:\n",
    "            break\n",
    "    important_value = max_min(important_value)\n",
    "    important_values = important_values + important_value\n",
    "X_lable = ['0','T','P','Q','R','S','T','P','']\n",
    "fig,ax = plt.subplots(1,1)\n",
    "ax.plot(np.arange(important_values.flatten().__len__()), max_min(important_values.flatten()), lw=1,color='k')\n",
    "ax.axvline(x=0,ls='dashed',lw=0.7,color='r')\n",
    "ax.axvline(x=10,ls='dashed',lw=0.7,color='g')\n",
    "ax.axvline(x=20,ls='dashed',lw=0.7,color='b')\n",
    "ax.axvline(x=30,ls='dashed',lw=0.7,color='c')\n",
    "ax.axvline(x=40,ls='dashed',lw=0.7,color='m')\n",
    "ax.axvline(x=50,ls='dashed',lw=0.7,color='r')\n",
    "ax.axvline(x=60,ls='dashed',lw=0.7,color='g')\n",
    "ax.set_xlim(-5,65)\n",
    "ax.set_xticklabels(X_lable)\n",
    "plt.savefig(save_root+'/HTN.png',dpi = 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 0\n",
    "segement_number = 10\n",
    "important_values = np.zeros([6,segement_number])\n",
    "\n",
    "\n",
    "for index in range(test_dataset.__len__()):#\n",
    "    important_value = np.zeros([6,segement_number])\n",
    "    inputs,labels = test_dataset.__getitem__(index)\n",
    "    if(labels != y_true[index]): \n",
    "        break\n",
    "    pred = y_pred[index]\n",
    "    if(pred == 1 or (labels == 1)): continue;\n",
    "    info =test_dataset.infos.iloc[index]\n",
    "    ECGfile_name = info['ECGFilename']\n",
    "    layervalue_file = layervalue_root  + ECGfile_name +'.npy'\n",
    "    waves_location_file_root = '/workspace/data/Preprocess_HTN/Waves_2023/'\n",
    "    waves = pd.read_csv(waves_location_file_root+ECGfile_name+'.csv',index_col=0)\n",
    "    if(waves.isnull().any().any()):\n",
    "        continue\n",
    "    layervalue = np.load(layervalue_file)\n",
    "\n",
    "    for head_peak in range(1,len(waves.columns)-1):\n",
    "        if(waves.loc['P'][head_peak] == waves.iloc[:,head_peak].min()): #P波不是第一个识别出来的波形\n",
    "            p_index = int(waves.loc['P'][head_peak])\n",
    "            q_index = int(waves.loc['Q'][head_peak])\n",
    "            r_index = int(waves.loc['R'][head_peak])\n",
    "            s_index = int(waves.loc['S'][head_peak])\n",
    "            t_index = int(waves.loc['T'][head_peak])\n",
    "            t_index_befort = int(waves.loc['T'][head_peak-1])\n",
    "            if(((waves.loc['P'][head_peak+1]==None)) or((waves.loc['P'][head_peak+1] < t_index))):\n",
    "                break\n",
    "            else:\n",
    "                p_index_next = waves.loc['P'][head_peak+1] \n",
    "            important_value[0] = important_value[0]+ layervalue_segement_count(t_index_befort,p_index,layervalue,segement_number)\n",
    "            important_value[1] = important_value[1]+ layervalue_segement_count(p_index,q_index,layervalue,segement_number)   \n",
    "            important_value[2] = important_value[2]+ layervalue_segement_count(q_index,r_index,layervalue,segement_number) \n",
    "            important_value[3] = important_value[3]+layervalue_segement_count(r_index,s_index,layervalue,segement_number)\n",
    "            important_value[4] = important_value[4]+ layervalue_segement_count(s_index,t_index,layervalue,segement_number) \n",
    "            important_value[5] = important_value[5]+ layervalue_segement_count(t_index,p_index_next,layervalue,segement_number) \n",
    "        else:\n",
    "            break\n",
    "    important_value = max_min(important_value)\n",
    "    important_values = important_values + important_value\n",
    "X_lable = ['0','T','P','Q','R','S','T','P','']\n",
    "fig,ax = plt.subplots(1,1)\n",
    "ax.plot(np.arange(important_values.flatten().__len__()), max_min(important_values.flatten()), lw=1,color='k')\n",
    "ax.axvline(x=0,ls='dashed',lw=0.7,color='r')\n",
    "ax.axvline(x=10,ls='dashed',lw=0.7,color='g')\n",
    "ax.axvline(x=20,ls='dashed',lw=0.7,color='b')\n",
    "ax.axvline(x=30,ls='dashed',lw=0.7,color='c')\n",
    "ax.axvline(x=40,ls='dashed',lw=0.7,color='m')\n",
    "ax.axvline(x=50,ls='dashed',lw=0.7,color='r')\n",
    "ax.axvline(x=60,ls='dashed',lw=0.7,color='g')\n",
    "ax.set_xlim(-5,65)\n",
    "ax.set_xticklabels(X_lable)\n",
    "plt.savefig(save_root+'/NHTN.png',dpi = 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 0\n",
    "segement_number = 10\n",
    "ECG_values = np.zeros([6,segement_number])\n",
    "\n",
    "\n",
    "for index in range(test_dataset.__len__()):#\n",
    "    ECG_value = np.zeros([6,segement_number])\n",
    "    inputs,labels = test_dataset.__getitem__(index)\n",
    "    info =test_dataset.infos.iloc[index]\n",
    "    ECGfile_name = info['ECGFilename']\n",
    "    layervalue_file = layervalue_root  + ECGfile_name +'.npy'\n",
    "    waves_location_file_root = '/workspace/data/Preprocess_HTN/Waves_2023/'\n",
    "    waves = pd.read_csv(waves_location_file_root+ECGfile_name+'.csv',index_col=0)\n",
    "    if(waves.isnull().any().any()):\n",
    "        continue\n",
    "    # layervalue = np.load(layervalue_file)\n",
    "    layervalue = np.array(inputs)\n",
    "    for head_peak in range(1,len(waves.columns)-1):\n",
    "        if(waves.loc['P'][head_peak] == waves.iloc[:,head_peak].min()): #P波不是第一个识别出来的波形\n",
    "            p_index = int(waves.loc['P'][head_peak])\n",
    "            q_index = int(waves.loc['Q'][head_peak])\n",
    "            r_index = int(waves.loc['R'][head_peak])\n",
    "            s_index = int(waves.loc['S'][head_peak])\n",
    "            t_index = int(waves.loc['T'][head_peak])\n",
    "            t_index_befort = int(waves.loc['T'][head_peak-1])\n",
    "            if(((waves.loc['P'][head_peak+1]==None)) or((waves.loc['P'][head_peak+1] < t_index))):\n",
    "                break\n",
    "            else:\n",
    "                p_index_next = waves.loc['P'][head_peak+1] \n",
    "            ECG_value[0] = ECG_value[0]+ layervalue_segement_count(t_index_befort,p_index,layervalue,segement_number)\n",
    "            ECG_value[1] = ECG_value[1]+ layervalue_segement_count(p_index,q_index,layervalue,segement_number)   \n",
    "            ECG_value[2] = ECG_value[2]+ layervalue_segement_count(q_index,r_index,layervalue,segement_number) \n",
    "            ECG_value[3] = ECG_value[3]+layervalue_segement_count(r_index,s_index,layervalue,segement_number)\n",
    "            ECG_value[4] = ECG_value[4]+ layervalue_segement_count(s_index,t_index,layervalue,segement_number) \n",
    "            ECG_value[5] = ECG_value[5]+ layervalue_segement_count(t_index,p_index_next,layervalue,segement_number) \n",
    "        else:\n",
    "            break\n",
    "    # ECG_value = max_min(ECG_value)\n",
    "    ECG_values = ECG_values + ECG_value\n",
    "X_lable = ['0','T','P','Q','R','S','T','P','']\n",
    "fig,ax = plt.subplots(1,1)\n",
    "ax.plot(np.arange(ECG_values.flatten().__len__()), max_min(ECG_values.flatten()), lw=1,color='k')\n",
    "ax.axvline(x=0,ls='dashed',lw=0.7,color='r')\n",
    "ax.axvline(x=10,ls='dashed',lw=0.7,color='g')\n",
    "ax.axvline(x=20,ls='dashed',lw=0.7,color='b')\n",
    "ax.axvline(x=30,ls='dashed',lw=0.7,color='c')\n",
    "ax.axvline(x=40,ls='dashed',lw=0.7,color='m')\n",
    "ax.axvline(x=50,ls='dashed',lw=0.7,color='r')\n",
    "ax.axvline(x=60,ls='dashed',lw=0.7,color='g')\n",
    "ax.set_xlim(-5,65)\n",
    "ax.set_xticklabels(X_lable)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 获取各个导联的心拍曲线"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def leads_layervalue_segement_count(start_index:int,end_index:int,layervalue:np.ndarray,segement_num:int = 10):\n",
    "    len_ = end_index-start_index\n",
    "    chanle_num = layervalue.shape[0]\n",
    "    layervalue_sum = np.zeros([chanle_num,segement_num])\n",
    "    if(len_<segement_num):\n",
    "        for i in range(segement_num):\n",
    "           layervalue_sum[:,i] = layervalue_sum[:,i] + ((layervalue[:,start_index:end_index].sum(axis = 1))/(segement_num*1.0)) #返回平均值，广播效应\n",
    "        return layervalue_sum\n",
    "    step = len_//segement_num\n",
    "    for i in range(segement_num):\n",
    "        start = start_index+step*i\n",
    "        sum_sement = layervalue[:,start:(start+step)].sum(axis = 1)/(chanle_num*step*1.0)\n",
    "        layervalue_sum[:,i] = layervalue_sum[:,i]+sum_sement*1. #在这一段step里面，每个时间点的平均重要程度\n",
    "    return layervalue_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 0\n",
    "segement_num = 10\n",
    "NHTN_important_values = np.zeros([6,12,segement_num])\n",
    "NHTN_ECG_values = np.zeros([6,12,segement_num])\n",
    "index_count= 0\n",
    "for index in range(test_dataset.__len__()):#\n",
    "    \n",
    "    inputs,labels = test_dataset.__getitem__(index)\n",
    "    if(labels == 1): continue\n",
    "    info =test_dataset.infos.iloc[index]\n",
    "    ECGfile_name = info['ECGFilename']\n",
    "    layervalue_file = layervalue_root  + ECGfile_name +'.npy'\n",
    "    waves_location_file_root = '/workspace/data/Preprocess_HTN/Waves_2023/'\n",
    "    waves = pd.read_csv(waves_location_file_root+ECGfile_name+'.csv',index_col=0)\n",
    "    if(waves.isnull().any().any()):\n",
    "        continue\n",
    "    layervalue = np.load(layervalue_file)\n",
    "    peak_count = 0\n",
    "    important_value = np.zeros([6,12,segement_num])\n",
    "    ECG_value = np.zeros([6,12,segement_num])\n",
    "    for head_peak in range(1,len(waves.columns)-1):\n",
    "        if(waves.loc['P'][head_peak] == waves.iloc[:,head_peak].min()): #P波不是第一个识别出来的波形\n",
    "            p_index = int(waves.loc['P'][head_peak])\n",
    "            q_index = int(waves.loc['Q'][head_peak])\n",
    "            r_index = int(waves.loc['R'][head_peak])\n",
    "            s_index = int(waves.loc['S'][head_peak])\n",
    "            t_index = int(waves.loc['T'][head_peak])\n",
    "            t_index_befort = int(waves.loc['T'][head_peak-1])\n",
    "            if(((waves.loc['P'][head_peak+1]==None)) or((waves.loc['P'][head_peak+1] < t_index))):\n",
    "                break\n",
    "            else:\n",
    "                p_index_next = waves.loc['P'][head_peak+1] \n",
    "        else:\n",
    "            break    \n",
    "        \n",
    "        important_value[0] = important_value[0]+ leads_layervalue_segement_count(t_index_befort,p_index,layervalue,segement_num)\n",
    "        important_value[1] = important_value[1]+ leads_layervalue_segement_count(p_index,q_index,layervalue,segement_num)   \n",
    "        important_value[2] = important_value[2]+ leads_layervalue_segement_count(q_index,r_index,layervalue,segement_num) \n",
    "        important_value[3] = important_value[3]+leads_layervalue_segement_count(r_index,s_index,layervalue,segement_num)\n",
    "        important_value[4] = important_value[4]+ leads_layervalue_segement_count(s_index,t_index,layervalue,segement_num) \n",
    "        important_value[5] = important_value[5]+ leads_layervalue_segement_count(t_index,p_index_next,layervalue,segement_num) \n",
    "        \n",
    "        \n",
    "        ECG_value[0] = ECG_value[0]+ leads_layervalue_segement_count(t_index_befort,p_index,layervalue,segement_num)\n",
    "        ECG_value[1] = ECG_value[1]+ leads_layervalue_segement_count(p_index,q_index,layervalue,segement_num)   \n",
    "        ECG_value[2] = ECG_value[2]+ leads_layervalue_segement_count(q_index,r_index,layervalue,segement_num) \n",
    "        ECG_value[3] = ECG_value[3]+leads_layervalue_segement_count(r_index,s_index,layervalue,segement_num)\n",
    "        ECG_value[4] = ECG_value[4]+ leads_layervalue_segement_count(s_index,t_index,layervalue,segement_num) \n",
    "        ECG_value[5] = ECG_value[5]+ leads_layervalue_segement_count(t_index,p_index_next,layervalue,segement_num) \n",
    "        peak_count = peak_count+1\n",
    "    \n",
    "    ECG_value = ECG_value/peak_count\n",
    "    important_value = important_value/peak_count\n",
    "    \n",
    "    important_value = max_min(important_value)\n",
    "    NHTN_important_values = NHTN_important_values + important_value\n",
    "    NHTN_ECG_values = NHTN_ECG_values +ECG_value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 0\n",
    "segement_num = 10\n",
    "HTN_important_values = np.zeros([6,12,segement_num])\n",
    "HTN_ECG_values = np.zeros([6,12,segement_num])\n",
    "index_count= 0\n",
    "for index in range(test_dataset.__len__()):#\n",
    "    \n",
    "    inputs,labels = test_dataset.__getitem__(index)\n",
    "    if(labels == 0): continue\n",
    "    info =test_dataset.infos.iloc[index]\n",
    "    ECGfile_name = info['ECGFilename']\n",
    "    layervalue_file = layervalue_root  + ECGfile_name +'.npy'\n",
    "    waves_location_file_root = '/workspace/data/Preprocess_HTN/Waves_2023/'\n",
    "    waves = pd.read_csv(waves_location_file_root+ECGfile_name+'.csv',index_col=0)\n",
    "    if(waves.isnull().any().any()):\n",
    "        continue\n",
    "    layervalue = np.load(layervalue_file)\n",
    "    peak_count = 0\n",
    "    important_value = np.zeros([6,12,segement_num])\n",
    "    ECG_value = np.zeros([6,12,segement_num])\n",
    "    for head_peak in range(1,len(waves.columns)-1):\n",
    "        if(waves.loc['P'][head_peak] == waves.iloc[:,head_peak].min()): #P波不是第一个识别出来的波形\n",
    "            p_index = int(waves.loc['P'][head_peak])\n",
    "            q_index = int(waves.loc['Q'][head_peak])\n",
    "            r_index = int(waves.loc['R'][head_peak])\n",
    "            s_index = int(waves.loc['S'][head_peak])\n",
    "            t_index = int(waves.loc['T'][head_peak])\n",
    "            t_index_befort = int(waves.loc['T'][head_peak-1])\n",
    "            if(((waves.loc['P'][head_peak+1]==None)) or((waves.loc['P'][head_peak+1] < t_index))):\n",
    "                break\n",
    "            else:\n",
    "                p_index_next = waves.loc['P'][head_peak+1] \n",
    "        else:\n",
    "            break    \n",
    "        \n",
    "        important_value[0] = important_value[0]+ leads_layervalue_segement_count(t_index_befort,p_index,layervalue,segement_num)\n",
    "        important_value[1] = important_value[1]+ leads_layervalue_segement_count(p_index,q_index,layervalue,segement_num)   \n",
    "        important_value[2] = important_value[2]+ leads_layervalue_segement_count(q_index,r_index,layervalue,segement_num) \n",
    "        important_value[3] = important_value[3]+leads_layervalue_segement_count(r_index,s_index,layervalue,segement_num)\n",
    "        important_value[4] = important_value[4]+ leads_layervalue_segement_count(s_index,t_index,layervalue,segement_num) \n",
    "        important_value[5] = important_value[5]+ leads_layervalue_segement_count(t_index,p_index_next,layervalue,segement_num) \n",
    "        \n",
    "        \n",
    "        ECG_value[0] = ECG_value[0]+ leads_layervalue_segement_count(t_index_befort,p_index,layervalue,segement_num)\n",
    "        ECG_value[1] = ECG_value[1]+ leads_layervalue_segement_count(p_index,q_index,layervalue,segement_num)   \n",
    "        ECG_value[2] = ECG_value[2]+ leads_layervalue_segement_count(q_index,r_index,layervalue,segement_num) \n",
    "        ECG_value[3] = ECG_value[3]+leads_layervalue_segement_count(r_index,s_index,layervalue,segement_num)\n",
    "        ECG_value[4] = ECG_value[4]+ leads_layervalue_segement_count(s_index,t_index,layervalue,segement_num) \n",
    "        ECG_value[5] = ECG_value[5]+ leads_layervalue_segement_count(t_index,p_index_next,layervalue,segement_num) \n",
    "        peak_count = peak_count+1\n",
    "    \n",
    "    ECG_value = ECG_value/peak_count\n",
    "    important_value = important_value/peak_count\n",
    "                \n",
    "    important_value = max_min(important_value)\n",
    "    HTN_important_values = HTN_important_values + important_value\n",
    "    HTN_ECG_values = HTN_ECG_values +ECG_value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lead_index = ['I', 'II', 'III', 'aVR', 'aVL', 'aVF', 'V1', 'V2', 'V3', 'V4', 'V5', 'V6']\n",
    "X_lable = ['T','P','Q','R','S','T','P']\n",
    "\n",
    "fig, axs = plt.subplots(nrows=3, ncols=4,sharey=True,figsize=(10,10), constrained_layout=True,dpi =100)\n",
    "\n",
    "cmap = 'OrRd' \n",
    "colors = ECGplot.color_map(important_values, cmap)#相当于归一化\n",
    "\n",
    "for i,ax in enumerate(axs.flat):  # type: ignore\n",
    "    HTN_plot_y = np.array(HTN_ECG_values[:,i,:].flatten())\n",
    "    HTN_colors_z = np.array(HTN_important_values[:,i,:].flatten())\n",
    "    NHTN_plot_y = np.array(NHTN_ECG_values[:,i,:].flatten())\n",
    "    NHTN_colors_z = np.array(NHTN_important_values[:,i,:].flatten())    \n",
    "    t = np.arange(0,HTN_plot_y.shape[0]) #  \n",
    "    ax.plot(t, max_min(HTN_colors_z), lw=1,color='r')\n",
    "    ax.plot(t, max_min(NHTN_colors_z), lw=1,color='b')\n",
    "    ax.axvline(x=0,ls='dashed',lw=0.7,color='k')\n",
    "    ax.axvline(x=10,ls='dashed',lw=0.7,color='k')\n",
    "    ax.axvline(x=20,ls='dashed',lw=0.7,color='k')\n",
    "    ax.axvline(x=30,ls='dashed',lw=0.7,color='k')\n",
    "    ax.axvline(x=40,ls='dashed',lw=0.7,color='k')\n",
    "    ax.axvline(x=50,ls='dashed',lw=0.7,color='k')\n",
    "    ax.axvline(x=60,ls='dashed',lw=0.7,color='k')\n",
    "    ax.set_xlim(-5,65)\n",
    "    \n",
    "    ax.set_xticks([0,10,20,30,40,50,60]) # 设置刻度\n",
    "    ax.set_xticklabels(X_lable)\n",
    "    ax.set_title(lead_index[i])\n",
    "    # for j in t:\n",
    "    #     ax.axvline(x=j,alpha=0.1,color=colors[i,j]) #第i导联 第j个时间点的importance value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lead_index = ['I', 'II', 'III', 'aVR', 'aVL', 'aVF', 'V1', 'V2', 'V3', 'V4', 'V5', 'V6']\n",
    "X_lable = ['0','T','P','Q','R','S','T','P','']\n",
    "\n",
    "x_index = np.arange(0,12)\n",
    "fig, axs = plt.subplots(nrows=3, ncols=4, sharex=True,sharey=True,figsize=(8,10), constrained_layout=True,dpi =100)\n",
    "ecg_data,_ = test_dataset.__getitem__(index)\n",
    "\n",
    "cmap = 'OrRd' \n",
    "colors = ECGplot.color_map(important_values, cmap)#相当于归一化\n",
    "\n",
    "for i,ax in enumerate(axs.flat):  # type: ignore\n",
    "    plot_y = np.array(ECG_values[i]*5000.)#缩放\n",
    "    t = np.arange(0,segement_length)\n",
    "    ECGplot.plot_ECG_line(fig,ax,x = t,y= plot_y,y_name = str(lead_index[i])+\" Voltage(mV)\",title = lead_index[i])\n",
    "    for j in t:\n",
    "        ax.axvline(x=j,alpha=0.1,color=colors[i,j]) #第i导联 第j个时间点的importance value"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 0\n",
    "segement_number = 10\n",
    "important_values = np.zeros([6,segement_number])\n",
    "Occlusion_root = save_root+'/Occlusion/'\n",
    "\n",
    "for index in range(test_dataset.__len__()):#\n",
    "    important_value = np.zeros([6,segement_number])\n",
    "    inputs,labels = test_dataset.__getitem__(index)\n",
    "    if(labels == 0) :continue\n",
    "    info =test_dataset.infos.iloc[index]\n",
    "    ECGfile_name = info['ECGFilename']\n",
    "    layervalue_file = Occlusion_root  + ECGfile_name +'.npy'\n",
    "    waves_location_file_root = '/workspace/data/Preprocess_HTN/Waves_2023/'\n",
    "    if(not os.path.exists(layervalue_file)):\n",
    "        print(layervalue_file,\" not exit\")\n",
    "        continue\n",
    "    waves = pd.read_csv(waves_location_file_root+ECGfile_name+'.csv',index_col=0)\n",
    "    if(waves.isnull().any().any()):\n",
    "        continue\n",
    "    layervalue = np.load(layervalue_file)\n",
    "\n",
    "    for head_peak in range(1,len(waves.columns)-1):\n",
    "        if(waves.loc['P'][head_peak] == waves.iloc[:,head_peak].min()): #P波不是第一个识别出来的波形\n",
    "            p_index = int(waves.loc['P'][head_peak])\n",
    "            q_index = int(waves.loc['Q'][head_peak])\n",
    "            r_index = int(waves.loc['R'][head_peak])\n",
    "            s_index = int(waves.loc['S'][head_peak])\n",
    "            t_index = int(waves.loc['T'][head_peak])\n",
    "            t_index_befort = int(waves.loc['T'][head_peak-1])\n",
    "            if(((waves.loc['P'][head_peak+1]==None)) or((waves.loc['P'][head_peak+1] < t_index))):\n",
    "                break \n",
    "            else:\n",
    "                p_index_next = waves.loc['P'][head_peak+1] \n",
    "            important_value[0] = important_value[0]+ layervalue_segement_count(t_index_befort,p_index,layervalue,segement_number)\n",
    "            important_value[1] = important_value[1]+ layervalue_segement_count(p_index,q_index,layervalue,segement_number)   \n",
    "            important_value[2] = important_value[2]+ layervalue_segement_count(q_index,r_index,layervalue,segement_number) \n",
    "            important_value[3] = important_value[3]+layervalue_segement_count(r_index,s_index,layervalue,segement_number)\n",
    "            important_value[4] = important_value[4]+ layervalue_segement_count(s_index,t_index,layervalue,segement_number) \n",
    "            important_value[5] = important_value[5]+ layervalue_segement_count(t_index,p_index_next,layervalue,segement_number) \n",
    "        else:\n",
    "            break\n",
    "    important_value = max_min(important_value)\n",
    "    important_values = important_values + important_value\n",
    "X_lable = ['0','T','P','Q','R','S','T','P','']\n",
    "fig,ax = plt.subplots(1,1)\n",
    "ax.plot(np.arange(important_values.flatten().__len__()), max_min(important_values.flatten()), lw=1,color='k')\n",
    "ax.axvline(x=0,ls='dashed',lw=0.7,color='r')\n",
    "ax.axvline(x=10,ls='dashed',lw=0.7,color='g')\n",
    "ax.axvline(x=20,ls='dashed',lw=0.7,color='b')\n",
    "ax.axvline(x=30,ls='dashed',lw=0.7,color='c')\n",
    "ax.axvline(x=40,ls='dashed',lw=0.7,color='m')\n",
    "ax.axvline(x=50,ls='dashed',lw=0.7,color='r')\n",
    "ax.axvline(x=60,ls='dashed',lw=0.7,color='g')\n",
    "ax.set_xlim(-5,65)\n",
    "ax.set_xticklabels(X_lable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 0\n",
    "segement_number = 10\n",
    "important_values = np.zeros([6,segement_number])\n",
    "Occlusion_root = save_root+'/Occlusion/'\n",
    "\n",
    "for index in range(test_dataset.__len__()):#\n",
    "    important_value = np.zeros([6,segement_number])\n",
    "    inputs,labels = test_dataset.__getitem__(index)\n",
    "    if(labels == 1) :continue\n",
    "    info =test_dataset.infos.iloc[index]\n",
    "    ECGfile_name = info['ECGFilename']\n",
    "    layervalue_file = Occlusion_root  + ECGfile_name +'.npy'\n",
    "    waves_location_file_root = '/workspace/data/Preprocess_HTN/Waves_2023/'\n",
    "    if(not os.path.exists(layervalue_file)):\n",
    "        print(layervalue_file,\" not exit\")\n",
    "        continue\n",
    "    waves = pd.read_csv(waves_location_file_root+ECGfile_name+'.csv',index_col=0)\n",
    "    if(waves.isnull().any().any()):\n",
    "        continue\n",
    "    layervalue = np.load(layervalue_file)\n",
    "\n",
    "    for head_peak in range(1,len(waves.columns)-1):\n",
    "        if(waves.loc['P'][head_peak] == waves.iloc[:,head_peak].min()): #P波不是第一个识别出来的波形\n",
    "            p_index = int(waves.loc['P'][head_peak])\n",
    "            q_index = int(waves.loc['Q'][head_peak])\n",
    "            r_index = int(waves.loc['R'][head_peak])\n",
    "            s_index = int(waves.loc['S'][head_peak])\n",
    "            t_index = int(waves.loc['T'][head_peak])\n",
    "            t_index_befort = int(waves.loc['T'][head_peak-1])\n",
    "            if(((waves.loc['P'][head_peak+1]==None)) or((waves.loc['P'][head_peak+1] < t_index))):\n",
    "                break\n",
    "            else:\n",
    "                p_index_next = waves.loc['P'][head_peak+1] \n",
    "            important_value[0] = important_value[0]+ layervalue_segement_count(t_index_befort,p_index,layervalue,segement_number)\n",
    "            important_value[1] = important_value[1]+ layervalue_segement_count(p_index,q_index,layervalue,segement_number)   \n",
    "            important_value[2] = important_value[2]+ layervalue_segement_count(q_index,r_index,layervalue,segement_number) \n",
    "            important_value[3] = important_value[3]+layervalue_segement_count(r_index,s_index,layervalue,segement_number)\n",
    "            important_value[4] = important_value[4]+ layervalue_segement_count(s_index,t_index,layervalue,segement_number) \n",
    "            important_value[5] = important_value[5]+ layervalue_segement_count(t_index,p_index_next,layervalue,segement_number) \n",
    "        else:\n",
    "            break\n",
    "    important_value = max_min(important_value)\n",
    "    important_values = important_values + important_value\n",
    "X_lable = ['0','T','P','Q','R','S','T','P','']\n",
    "fig,ax = plt.subplots(1,1)\n",
    "ax.plot(np.arange(important_values.flatten().__len__()), max_min(important_values.flatten()), lw=1,color='k')\n",
    "ax.axvline(x=0,ls='dashed',lw=0.7,color='r')\n",
    "ax.axvline(x=10,ls='dashed',lw=0.7,color='g')\n",
    "ax.axvline(x=20,ls='dashed',lw=0.7,color='b')\n",
    "ax.axvline(x=30,ls='dashed',lw=0.7,color='c')\n",
    "ax.axvline(x=40,ls='dashed',lw=0.7,color='m')\n",
    "ax.axvline(x=50,ls='dashed',lw=0.7,color='r')\n",
    "ax.axvline(x=60,ls='dashed',lw=0.7,color='g')\n",
    "ax.set_xlim(-5,65)\n",
    "ax.set_xticklabels(X_lable)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------------"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GRU：2 ATT  batchsize：64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array([0.3512900727135794, 0.3734808053289141, 0.3555836294378553, 0.3445756807923317, 0.34377362579107285]).mean() #logs/20230212_134040/log.log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array([0.8434311224489796, 0.8252060439560439, 0.852782392026578, 0.861328125, 0.864453125] ).mean() "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3 layer GRU  batchsize = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array([0.34137497203690664, 0.419583329132625, 0.2343021673815591, 0.3741154298186302, 0.2929470892995596]).mean()  #/logs/20230211_171822/log.log    3 layer GRU  batchsize = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array([0.8499453352769679, 0.823489010989011, 0.9052637043189369, 0.8460582386363636, 0.88359375]).mean()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array([0.9523809523809523,0.8523809523809524,0.9790575916230366,0.8393574297188755,0.8310344827586207]).mean()  #precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array([0.7339449541284404,0.7955555555555556,0.8385650224215246, 0.8565573770491803,0.8763636363636363]).mean()  #recall"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2 layer GRU  batchsize = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array([0.32283543795347214, 0.44184040278196335, 0.29647157341241837, 0.4000786542892456, 0.342408150434494]).mean() # vlogs/20230211_072050/log.log   2 layer GRU  batchsize = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array([0.8651147959183674, 0.794921875, 0.8811319040697674, 0.8361458333333334, 0.8776041666666666] ).mean() "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " 2 layer GRU  batchsize = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array( [0.3219981959887913, 0.3934356187071119, 0.28491889366081785, 0.33339295722544193, 0.3012485932558775]).mean()  # logs/20230212_031236/log.log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array([0.8693513119533528, 0.835679945054945, 0.8918708471760797, 0.869140625, 0.88828125] ).mean() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array([0.9497206703910615, 0.8768472906403941,0.9538461538461539,0.9447236180904522,0.8988326848249028]).mean()  #precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array( [0.7798165137614679,0.7911111111111111, 0.8340807174887892, 0.7704918032786885,0.8884615384615384]).mean()  #recall"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GradientShap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NET = [Net.MLBFNet_GUR_o(True,True,True,2,Dropout_rate=0.3) ] # type: ignore\n",
    "testmodel = NET[0].to(DEVICE)\n",
    "Models_path = '/workspace/data/Interpretable_HTN/model/20230212_031236/20230212_031236/parameter_EarlyStoping_4.pt'\n",
    "save_root = Models_path[:-3]+'/'\n",
    "layervalue_root = save_root+'/layervalue/'\n",
    "testmodel.load_state_dict(torch.load(Models_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(data,testmodel):\n",
    "    testmodel.eval()\n",
    "    output = testmodel(data)\n",
    "    porbs = torch.nn.functional.softmax(output)\n",
    "    return porbs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs,labels = test_dataset.__getitem__(index)\n",
    "info =test_dataset.infos.iloc[index]\n",
    "ECGfile_name = info['ECGFilename']\n",
    "testmodel.eval()\n",
    "inputs = inputs.unsqueeze(0)\n",
    "inputs = inputs.to(DEVICE)\n",
    "labels = labels.to(DEVICE)  \n",
    "predict(inputs,testmodel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from captum.attr import GradientShap\n",
    "from captum.attr import visualization as viz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gradient_shap = GradientShap(testmodel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rand_img_dist = torch.cat([inputs * 0, inputs * 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rand_img_dist.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attributions_gs = gradient_shap.attribute(inputs,\n",
    "                                          n_samples=50,\n",
    "                                          stdevs=0.0001,\n",
    "                                          baselines=rand_img_dist,\n",
    "                                          target=labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attributions_gs.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "遮挡可解释性分析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from captum.attr import Occlusion\n",
    "from captum.attr import visualization as viz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "occlusion = Occlusion(testmodel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_root = Models_path[:-3]+'/'\n",
    "Occlusion_root = save_root+'/Occlusion/'\n",
    "if(not(os.path.exists(save_root))): os.mkdir(save_root)\n",
    "if(not(os.path.exists(Occlusion_root))): os.mkdir(Occlusion_root)\n",
    "lead_index = ['I', 'II', 'III', 'aVR', 'aVL', 'aVF', 'V1', 'V2', 'V3', 'V4', 'V5', 'V6']\n",
    "x_index = np.arange(0,12)\n",
    "# fig, axs = plt.subplots(nrows=4, ncols=3, sharex=True,sharey=True,figsize=(45,25), constrained_layout=True,dpi =100)\n",
    "for index in tqdm(range(test_dataset.__len__())):#test_dataset.__len__()\n",
    "    \n",
    "    inputs,labels = test_dataset.__getitem__(index)\n",
    "    info =test_dataset.infos.iloc[index]\n",
    "    inputs = inputs.unsqueeze(0)\n",
    "    ECGfile_name = info['ECGFilename']\n",
    "    attributions_occ = occlusion.attribute(inputs,\n",
    "                                        strides = (1, 10), # 遮挡滑动移动步长\n",
    "                                        target=labels, # 目标类别\n",
    "                                        sliding_window_shapes=(12, 20), # 遮挡滑块尺寸\n",
    "                                        baselines=0)\n",
    "    Occlusion_vlue = attributions_occ.squeeze(0).numpy()\n",
    "    np.save(Occlusion_root+ECGfile_name,Occlusion_vlue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer2d_vlue = attributions_occ.squeeze(0).numpy()\n",
    "lead_index = ['I', 'II', 'III', 'aVR', 'aVL', 'aVF', 'V1', 'V2', 'V3', 'V4', 'V5', 'V6']\n",
    "x_index = np.arange(0,12)\n",
    "fig, axs = plt.subplots(nrows=4, ncols=3, sharex=True,sharey=True,figsize=(45,25), constrained_layout=True,dpi =100)\n",
    "ecg_data,_ = test_dataset.__getitem__(index)\n",
    "\n",
    "cmap = 'OrRd' \n",
    "colors = ECGplot.color_map(layer2d_vlue, cmap)#相当于归一化\n",
    "\n",
    "for i,ax in enumerate(axs.flat):  # type: ignore\n",
    "    plot_y = np.array(ecg_data[i]*5000.)#缩放\n",
    "    t = np.arange(0,5000)\n",
    "    ECGplot.plot_ECG_line(fig,ax,x = t,y= plot_y,y_name = str(lead_index[i])+\" Voltage(mV)\",title = lead_index[i])\n",
    "    for j in t:\n",
    "        ax.axvline(x=j,alpha=0.1,color=colors[i,j]) #第i导联 第j个时间点的importance value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13 (default, Mar 29 2022, 02:18:16) \n[GCC 7.5.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
