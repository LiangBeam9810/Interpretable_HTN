{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 1\n",
    "%aimport ecg_get_data\n",
    "%aimport Models\n",
    "%aimport train_test_validat\n",
    "%aimport self_attention\n",
    "%aimport ECGplot\n",
    "import torch\n",
    "import torchvision\n",
    "from torch.autograd import Variable\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "import Models \n",
    "from train_test_validat import *\n",
    "from self_attention import *\n",
    "from  ecg_get_data import *\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import time\n",
    "\n",
    "random_seed = 2\n",
    "torch.manual_seed(random_seed)    # reproducible\n",
    "torch.cuda.manual_seed_all(random_seed)\n",
    "random.seed(random_seed)\n",
    "np.random.seed(random_seed)\n",
    "\n",
    "time_str = time.strftime(\"%Y%m%d_%H%M%S\", time.localtime()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:1\n"
     ]
    }
   ],
   "source": [
    "data_path =  './PNG_Power/' #路径\n",
    "lable_path = './label.npy'\n",
    "model_path = './model/'+time_str\n",
    "log_path = './log/'+  time_str\n",
    "\n",
    "\n",
    "EcgChannles_num = 12\n",
    "EcgLength_num = 5000\n",
    "DEVICE = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data.dataset import Dataset\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "from torchvision.transforms import transforms\n",
    "from pathlib import Path\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, root, transform=None):\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.Resize([224, 224]),\n",
    "            #transforms.Resize(224),\n",
    "            #transforms.RandomResizedCrop((224,224)),\n",
    "            transforms.ToTensor(), # 将图片(Image)转成Tensor，归一化至[0, 1]（直接除以255）\n",
    "            transforms.Normalize(mean=[.5, .5, .5], std=[.5, .5, .5]) # 标准化至[-1, 1]，规定均值和标准差\n",
    "            #transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]) # 常用标准化\n",
    "\n",
    "        ])\n",
    "        images_path = Path(root)\n",
    "        images_list = list(images_path.glob('*.png')) # list(images_path.glob('*.png'))\n",
    "        \n",
    "        images_list_str = [ str(x) for x in images_list ]\n",
    "        #print(images_list_str)\n",
    "        images_list_str.sort(key=lambda x:int(((x.split('.')[0])).split('/')[-1])) #把'root/xx.png' 先按“.”分割，再按“/”分割，得到编号xx，转为整形并排序      #排序\n",
    "        self.images = images_list_str\n",
    "        #print(images_list_str)\n",
    "    def __getitem__(self, item):\n",
    "        image_path = self.images[item]\n",
    "        #print(image_path)\n",
    "        # image = cv2.imdecode(np.fromfile(image_path, dtype=np.uint8), 1) # 读到的是BGR数据\n",
    "        # image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) # 转化为RGB\n",
    "        # # 这时的image是H, W, C的顺序，因此下面需要转化为：C, H, W\n",
    "        # image = torch.from_numpy(image).permute(2, 0, 1)/255 # 归一化[0, 1]才能与PIL读取的数据一致\n",
    "        image_num = int(((image_path.split('.')[0])).split('/')[-1])-1  #把'root/xx.png' 先按“.”分割，再按“/”分割，得到编号xx，转为整形并排序      #排序\n",
    "        #print(image_num)\n",
    "        image = Image.open(image_path)  # 读取到的是RGBA\n",
    "        #print(image.format, image.size, image.mode)#打印出原图格式\n",
    "        image = image.convert(\"RGB\") \n",
    "        #print(image.format, image.size, image.mode)#打印出现在图格式\n",
    "        image = self.transform(image)   # transform转化image为：C, H, W\n",
    "        label = 1 if image_num<689 else 0 # 这是一个label的示例，可自定义\n",
    "        return image, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "    def get_size(self):\n",
    "        image_path = self.images[0]\n",
    "        image = Image.open(image_path)\n",
    "        image = image.convert(\"RGB\") \n",
    "        image = self.transform(image)\n",
    "        return image.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = MyDataset(os.path.join(data_path,'train/'))\n",
    "validate_date = MyDataset(os.path.join(data_path,'valid/'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validate_date.__len__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.__len__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1036/1395949848.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_size\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_1036/2492757226.py\u001b[0m in \u001b[0;36mget_size\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_size\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m         \u001b[0mimage_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m         \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"RGB\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "train_data.get_size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "class Bottleneck(nn.Module):\n",
    "    #每个stage维度中扩展的倍数\n",
    "    extention=4\n",
    "    def __init__(self,inplanes,planes,stride,downsample=None):\n",
    "        '''\n",
    "\n",
    "        :param inplanes: 输入block的之前的通道数\n",
    "        :param planes: 在block中间处理的时候的通道数\n",
    "                planes*self.extention:输出的维度\n",
    "        :param stride:\n",
    "        :param downsample:\n",
    "        '''\n",
    "        super(Bottleneck, self).__init__()\n",
    "\n",
    "        self.conv1=nn.Conv2d(inplanes,planes,kernel_size=1,stride=stride,bias=False)\n",
    "        self.bn1=nn.BatchNorm2d(planes)\n",
    "\n",
    "        self.conv2=nn.Conv2d(planes,planes,kernel_size=3,stride=1,padding=1,bias=False)\n",
    "        self.bn2=nn.BatchNorm2d(planes)\n",
    "\n",
    "        self.conv3=nn.Conv2d(planes,planes*self.extention,kernel_size=1,stride=1,bias=False)\n",
    "        self.bn3=nn.BatchNorm2d(planes*self.extention)\n",
    "\n",
    "        self.relu=nn.ReLU(inplace=True)\n",
    "\n",
    "        #判断残差有没有卷积\n",
    "        self.downsample=downsample\n",
    "        self.stride=stride\n",
    "\n",
    "    def forward(self,x):\n",
    "        #参差数据\n",
    "        residual=x\n",
    "\n",
    "        #卷积操作\n",
    "        out=self.conv1(x)\n",
    "        out=self.bn1(out)\n",
    "        out=self.relu(out)\n",
    "\n",
    "        out=self.conv2(out)\n",
    "        out=self.bn2(out)\n",
    "        out=self.relu(out)\n",
    "\n",
    "        out=self.conv3(out)\n",
    "        out=self.bn3(out)\n",
    "        out=self.relu(out)\n",
    "\n",
    "        #是否直连（如果Indentity blobk就是直连；如果Conv2 Block就需要对残差边就行卷积，改变通道数和size\n",
    "        if self.downsample is not None:\n",
    "            residual=self.downsample(x)\n",
    "\n",
    "        #将残差部分和卷积部分相加\n",
    "        out=residual+out\n",
    "        out=self.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self,block,layers,num_class):\n",
    "        #inplane=当前的fm的通道数\n",
    "        self.inplane=64\n",
    "        super(ResNet, self).__init__()\n",
    "\n",
    "        #参数\n",
    "        self.block=block\n",
    "        self.layers=layers\n",
    "\n",
    "        #stem的网络层\n",
    "        self.conv1=nn.Conv2d(3,self.inplane,kernel_size=7,stride=2,padding=3,bias=False)\n",
    "        self.bn1=nn.BatchNorm2d(self.inplane)\n",
    "        self.relu=nn.ReLU()\n",
    "        self.maxpool=nn.MaxPool2d(kernel_size=3,stride=2,padding=1)\n",
    "\n",
    "        #64,128,256,512指的是扩大4倍之前的维度，即Identity Block中间的维度\n",
    "        self.stage1=self.make_layer(self.block,64,layers[0],stride=1)\n",
    "        self.stage2=self.make_layer(self.block,128,layers[1],stride=2)\n",
    "        self.stage3=self.make_layer(self.block,256,layers[2],stride=2)\n",
    "        self.stage4=self.make_layer(self.block,512,layers[3],stride=2)\n",
    "\n",
    "        #后续的网络\n",
    "        self.avgpool=nn.AvgPool2d(7)\n",
    "        self.fc=nn.Linear(512*block.extention,num_class)\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "        self.dropout = torch.nn.Dropout(0.5)\n",
    "    def forward(self,x):\n",
    "        #stem部分：conv+bn+maxpool\n",
    "        out=self.conv1(x)\n",
    "        out=self.bn1(out)\n",
    "        out=self.relu(out)\n",
    "        out=self.maxpool(out)\n",
    "        #print(out.shape)\n",
    "\n",
    "        #block部分\n",
    "        out=self.stage1(out)\n",
    "        #print(out.shape)\n",
    "        out=self.stage2(out)\n",
    "        #print(out.shape)\n",
    "        out=self.stage3(out)\n",
    "        #print(out.shape)\n",
    "        out=self.dropout(self.stage4(out))\n",
    "        #print(out.shape)\n",
    "        #分类\n",
    "        out=self.avgpool(out)\n",
    "        #print(out.shape)\n",
    "        out=torch.flatten(out,1)\n",
    "        #print(out.shape)\n",
    "        out=self.fc(out)\n",
    "        out=self.softmax(out)\n",
    "        return out\n",
    "\n",
    "    def make_layer(self,block,plane,block_num,stride=1):\n",
    "        '''\n",
    "        :param block: block模板\n",
    "        :param plane: 每个模块中间运算的维度，一般等于输出维度/4\n",
    "        :param block_num: 重复次数\n",
    "        :param stride: 步长\n",
    "        :return:\n",
    "        '''\n",
    "        block_list=[]\n",
    "        #先计算要不要加downsample\n",
    "        downsample=None\n",
    "        if(stride!=1 or self.inplane!=plane*block.extention):\n",
    "            downsample=nn.Sequential(\n",
    "                nn.Conv2d(self.inplane,plane*block.extention,stride=stride,kernel_size=1,bias=False),\n",
    "                nn.BatchNorm2d(plane*block.extention)\n",
    "            )\n",
    "\n",
    "        # Conv Block输入和输出的维度（通道数和size）是不一样的，所以不能连续串联，他的作用是改变网络的维度\n",
    "        # Identity Block 输入维度和输出（通道数和size）相同，可以直接串联，用于加深网络\n",
    "        #Conv_block\n",
    "        conv_block=block(self.inplane,plane,stride=stride,downsample=downsample)\n",
    "        block_list.append(conv_block)\n",
    "        self.inplane=plane*block.extention\n",
    "\n",
    "        #Identity Block\n",
    "        for i in range(1,block_num):\n",
    "            block_list.append(block(self.inplane,plane,stride=1))\n",
    "\n",
    "        return nn.Sequential(*block_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "FOLDS = 1\n",
    "EPOCHS = 5000  \n",
    "PATIENCE = 36\n",
    "LR = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter   \n",
    "os.makedirs(model_path, exist_ok=True)\n",
    "writer = SummaryWriter(log_path)\n",
    "from torchsummary import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 64, 112, 112]           9,408\n",
      "       BatchNorm2d-2         [-1, 64, 112, 112]             128\n",
      "              ReLU-3         [-1, 64, 112, 112]               0\n",
      "         MaxPool2d-4           [-1, 64, 56, 56]               0\n",
      "            Conv2d-5           [-1, 64, 56, 56]           4,096\n",
      "       BatchNorm2d-6           [-1, 64, 56, 56]             128\n",
      "              ReLU-7           [-1, 64, 56, 56]               0\n",
      "            Conv2d-8           [-1, 64, 56, 56]          36,864\n",
      "       BatchNorm2d-9           [-1, 64, 56, 56]             128\n",
      "             ReLU-10           [-1, 64, 56, 56]               0\n",
      "           Conv2d-11          [-1, 256, 56, 56]          16,384\n",
      "      BatchNorm2d-12          [-1, 256, 56, 56]             512\n",
      "             ReLU-13          [-1, 256, 56, 56]               0\n",
      "           Conv2d-14          [-1, 256, 56, 56]          16,384\n",
      "      BatchNorm2d-15          [-1, 256, 56, 56]             512\n",
      "             ReLU-16          [-1, 256, 56, 56]               0\n",
      "       Bottleneck-17          [-1, 256, 56, 56]               0\n",
      "           Conv2d-18           [-1, 64, 56, 56]          16,384\n",
      "      BatchNorm2d-19           [-1, 64, 56, 56]             128\n",
      "             ReLU-20           [-1, 64, 56, 56]               0\n",
      "           Conv2d-21           [-1, 64, 56, 56]          36,864\n",
      "      BatchNorm2d-22           [-1, 64, 56, 56]             128\n",
      "             ReLU-23           [-1, 64, 56, 56]               0\n",
      "           Conv2d-24          [-1, 256, 56, 56]          16,384\n",
      "      BatchNorm2d-25          [-1, 256, 56, 56]             512\n",
      "             ReLU-26          [-1, 256, 56, 56]               0\n",
      "             ReLU-27          [-1, 256, 56, 56]               0\n",
      "       Bottleneck-28          [-1, 256, 56, 56]               0\n",
      "           Conv2d-29           [-1, 64, 56, 56]          16,384\n",
      "      BatchNorm2d-30           [-1, 64, 56, 56]             128\n",
      "             ReLU-31           [-1, 64, 56, 56]               0\n",
      "           Conv2d-32           [-1, 64, 56, 56]          36,864\n",
      "      BatchNorm2d-33           [-1, 64, 56, 56]             128\n",
      "             ReLU-34           [-1, 64, 56, 56]               0\n",
      "           Conv2d-35          [-1, 256, 56, 56]          16,384\n",
      "      BatchNorm2d-36          [-1, 256, 56, 56]             512\n",
      "             ReLU-37          [-1, 256, 56, 56]               0\n",
      "             ReLU-38          [-1, 256, 56, 56]               0\n",
      "       Bottleneck-39          [-1, 256, 56, 56]               0\n",
      "           Conv2d-40          [-1, 128, 28, 28]          32,768\n",
      "      BatchNorm2d-41          [-1, 128, 28, 28]             256\n",
      "             ReLU-42          [-1, 128, 28, 28]               0\n",
      "           Conv2d-43          [-1, 128, 28, 28]         147,456\n",
      "      BatchNorm2d-44          [-1, 128, 28, 28]             256\n",
      "             ReLU-45          [-1, 128, 28, 28]               0\n",
      "           Conv2d-46          [-1, 512, 28, 28]          65,536\n",
      "      BatchNorm2d-47          [-1, 512, 28, 28]           1,024\n",
      "             ReLU-48          [-1, 512, 28, 28]               0\n",
      "           Conv2d-49          [-1, 512, 28, 28]         131,072\n",
      "      BatchNorm2d-50          [-1, 512, 28, 28]           1,024\n",
      "             ReLU-51          [-1, 512, 28, 28]               0\n",
      "       Bottleneck-52          [-1, 512, 28, 28]               0\n",
      "           Conv2d-53          [-1, 128, 28, 28]          65,536\n",
      "      BatchNorm2d-54          [-1, 128, 28, 28]             256\n",
      "             ReLU-55          [-1, 128, 28, 28]               0\n",
      "           Conv2d-56          [-1, 128, 28, 28]         147,456\n",
      "      BatchNorm2d-57          [-1, 128, 28, 28]             256\n",
      "             ReLU-58          [-1, 128, 28, 28]               0\n",
      "           Conv2d-59          [-1, 512, 28, 28]          65,536\n",
      "      BatchNorm2d-60          [-1, 512, 28, 28]           1,024\n",
      "             ReLU-61          [-1, 512, 28, 28]               0\n",
      "             ReLU-62          [-1, 512, 28, 28]               0\n",
      "       Bottleneck-63          [-1, 512, 28, 28]               0\n",
      "           Conv2d-64          [-1, 128, 28, 28]          65,536\n",
      "      BatchNorm2d-65          [-1, 128, 28, 28]             256\n",
      "             ReLU-66          [-1, 128, 28, 28]               0\n",
      "           Conv2d-67          [-1, 128, 28, 28]         147,456\n",
      "      BatchNorm2d-68          [-1, 128, 28, 28]             256\n",
      "             ReLU-69          [-1, 128, 28, 28]               0\n",
      "           Conv2d-70          [-1, 512, 28, 28]          65,536\n",
      "      BatchNorm2d-71          [-1, 512, 28, 28]           1,024\n",
      "             ReLU-72          [-1, 512, 28, 28]               0\n",
      "             ReLU-73          [-1, 512, 28, 28]               0\n",
      "       Bottleneck-74          [-1, 512, 28, 28]               0\n",
      "           Conv2d-75          [-1, 128, 28, 28]          65,536\n",
      "      BatchNorm2d-76          [-1, 128, 28, 28]             256\n",
      "             ReLU-77          [-1, 128, 28, 28]               0\n",
      "           Conv2d-78          [-1, 128, 28, 28]         147,456\n",
      "      BatchNorm2d-79          [-1, 128, 28, 28]             256\n",
      "             ReLU-80          [-1, 128, 28, 28]               0\n",
      "           Conv2d-81          [-1, 512, 28, 28]          65,536\n",
      "      BatchNorm2d-82          [-1, 512, 28, 28]           1,024\n",
      "             ReLU-83          [-1, 512, 28, 28]               0\n",
      "             ReLU-84          [-1, 512, 28, 28]               0\n",
      "       Bottleneck-85          [-1, 512, 28, 28]               0\n",
      "           Conv2d-86          [-1, 256, 14, 14]         131,072\n",
      "      BatchNorm2d-87          [-1, 256, 14, 14]             512\n",
      "             ReLU-88          [-1, 256, 14, 14]               0\n",
      "           Conv2d-89          [-1, 256, 14, 14]         589,824\n",
      "      BatchNorm2d-90          [-1, 256, 14, 14]             512\n",
      "             ReLU-91          [-1, 256, 14, 14]               0\n",
      "           Conv2d-92         [-1, 1024, 14, 14]         262,144\n",
      "      BatchNorm2d-93         [-1, 1024, 14, 14]           2,048\n",
      "             ReLU-94         [-1, 1024, 14, 14]               0\n",
      "           Conv2d-95         [-1, 1024, 14, 14]         524,288\n",
      "      BatchNorm2d-96         [-1, 1024, 14, 14]           2,048\n",
      "             ReLU-97         [-1, 1024, 14, 14]               0\n",
      "       Bottleneck-98         [-1, 1024, 14, 14]               0\n",
      "           Conv2d-99          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-100          [-1, 256, 14, 14]             512\n",
      "            ReLU-101          [-1, 256, 14, 14]               0\n",
      "          Conv2d-102          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-103          [-1, 256, 14, 14]             512\n",
      "            ReLU-104          [-1, 256, 14, 14]               0\n",
      "          Conv2d-105         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-106         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-107         [-1, 1024, 14, 14]               0\n",
      "            ReLU-108         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-109         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-110          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-111          [-1, 256, 14, 14]             512\n",
      "            ReLU-112          [-1, 256, 14, 14]               0\n",
      "          Conv2d-113          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-114          [-1, 256, 14, 14]             512\n",
      "            ReLU-115          [-1, 256, 14, 14]               0\n",
      "          Conv2d-116         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-117         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-118         [-1, 1024, 14, 14]               0\n",
      "            ReLU-119         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-120         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-121          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-122          [-1, 256, 14, 14]             512\n",
      "            ReLU-123          [-1, 256, 14, 14]               0\n",
      "          Conv2d-124          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-125          [-1, 256, 14, 14]             512\n",
      "            ReLU-126          [-1, 256, 14, 14]               0\n",
      "          Conv2d-127         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-128         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-129         [-1, 1024, 14, 14]               0\n",
      "            ReLU-130         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-131         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-132          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-133          [-1, 256, 14, 14]             512\n",
      "            ReLU-134          [-1, 256, 14, 14]               0\n",
      "          Conv2d-135          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-136          [-1, 256, 14, 14]             512\n",
      "            ReLU-137          [-1, 256, 14, 14]               0\n",
      "          Conv2d-138         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-139         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-140         [-1, 1024, 14, 14]               0\n",
      "            ReLU-141         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-142         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-143          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-144          [-1, 256, 14, 14]             512\n",
      "            ReLU-145          [-1, 256, 14, 14]               0\n",
      "          Conv2d-146          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-147          [-1, 256, 14, 14]             512\n",
      "            ReLU-148          [-1, 256, 14, 14]               0\n",
      "          Conv2d-149         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-150         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-151         [-1, 1024, 14, 14]               0\n",
      "            ReLU-152         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-153         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-154            [-1, 512, 7, 7]         524,288\n",
      "     BatchNorm2d-155            [-1, 512, 7, 7]           1,024\n",
      "            ReLU-156            [-1, 512, 7, 7]               0\n",
      "          Conv2d-157            [-1, 512, 7, 7]       2,359,296\n",
      "     BatchNorm2d-158            [-1, 512, 7, 7]           1,024\n",
      "            ReLU-159            [-1, 512, 7, 7]               0\n",
      "          Conv2d-160           [-1, 2048, 7, 7]       1,048,576\n",
      "     BatchNorm2d-161           [-1, 2048, 7, 7]           4,096\n",
      "            ReLU-162           [-1, 2048, 7, 7]               0\n",
      "          Conv2d-163           [-1, 2048, 7, 7]       2,097,152\n",
      "     BatchNorm2d-164           [-1, 2048, 7, 7]           4,096\n",
      "            ReLU-165           [-1, 2048, 7, 7]               0\n",
      "      Bottleneck-166           [-1, 2048, 7, 7]               0\n",
      "          Conv2d-167            [-1, 512, 7, 7]       1,048,576\n",
      "     BatchNorm2d-168            [-1, 512, 7, 7]           1,024\n",
      "            ReLU-169            [-1, 512, 7, 7]               0\n",
      "          Conv2d-170            [-1, 512, 7, 7]       2,359,296\n",
      "     BatchNorm2d-171            [-1, 512, 7, 7]           1,024\n",
      "            ReLU-172            [-1, 512, 7, 7]               0\n",
      "          Conv2d-173           [-1, 2048, 7, 7]       1,048,576\n",
      "     BatchNorm2d-174           [-1, 2048, 7, 7]           4,096\n",
      "            ReLU-175           [-1, 2048, 7, 7]               0\n",
      "            ReLU-176           [-1, 2048, 7, 7]               0\n",
      "      Bottleneck-177           [-1, 2048, 7, 7]               0\n",
      "          Conv2d-178            [-1, 512, 7, 7]       1,048,576\n",
      "     BatchNorm2d-179            [-1, 512, 7, 7]           1,024\n",
      "            ReLU-180            [-1, 512, 7, 7]               0\n",
      "          Conv2d-181            [-1, 512, 7, 7]       2,359,296\n",
      "     BatchNorm2d-182            [-1, 512, 7, 7]           1,024\n",
      "            ReLU-183            [-1, 512, 7, 7]               0\n",
      "          Conv2d-184           [-1, 2048, 7, 7]       1,048,576\n",
      "     BatchNorm2d-185           [-1, 2048, 7, 7]           4,096\n",
      "            ReLU-186           [-1, 2048, 7, 7]               0\n",
      "            ReLU-187           [-1, 2048, 7, 7]               0\n",
      "      Bottleneck-188           [-1, 2048, 7, 7]               0\n",
      "         Dropout-189           [-1, 2048, 7, 7]               0\n",
      "       AvgPool2d-190           [-1, 2048, 1, 1]               0\n",
      "          Linear-191                    [-1, 2]           4,098\n",
      "         Softmax-192                    [-1, 2]               0\n",
      "================================================================\n",
      "Total params: 23,512,130\n",
      "Trainable params: 23,512,130\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.57\n",
      "Forward/backward pass size (MB): 317.37\n",
      "Params size (MB): 89.69\n",
      "Estimated Total Size (MB): 407.63\n",
      "----------------------------------------------------------------\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "NET = [ResNet(Bottleneck,[3,4,6,3],2) for i in range(FOLDS)]\n",
    "print(summary(NET[0], (3, 224, 224), device=\"cpu\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Epoch: 1 - Train_loss: 0.70071 - Train_acc: 0.59784 - Val_loss: 0.71272 - Val_acc: 0.602679 - T_Time: 42.296\n",
      "当前学习率：0.001000\n",
      "Find better model in Epoch 1, saving model.\n",
      "Validation loss decreased (inf --> 0.712720).  Saving model ...\n",
      "                    --------------------------------------------------\n",
      "\n",
      "- Epoch: 2 - Train_loss: 0.62686 - Train_acc: 0.67354 - Val_loss: 0.74408 - Val_acc: 0.549107 - T_Time: 40.325\n",
      "当前学习率：0.001000\n",
      "EarlyStopping counter: 1 out of 36\n",
      "\n",
      "- Epoch: 3 - Train_loss: 0.64608 - Train_acc: 0.64750 - Val_loss: 0.75967 - Val_acc: 0.553571 - T_Time: 40.270\n",
      "当前学习率：0.001000\n",
      "EarlyStopping counter: 2 out of 36\n",
      "\n",
      "- Epoch: 4 - Train_loss: 0.71237 - Train_acc: 0.60050 - Val_loss: 0.77308 - Val_acc: 0.540179 - T_Time: 40.674\n",
      "当前学习率：0.001000\n",
      "EarlyStopping counter: 3 out of 36\n",
      "\n",
      "- Epoch: 5 - Train_loss: 0.71256 - Train_acc: 0.60071 - Val_loss: 0.79987 - Val_acc: 0.513393 - T_Time: 40.460\n",
      "当前学习率：0.001000\n",
      "EarlyStopping counter: 4 out of 36\n",
      "\n",
      "- Epoch: 6 - Train_loss: 0.71240 - Train_acc: 0.60086 - Val_loss: 0.77308 - Val_acc: 0.540179 - T_Time: 40.478\n",
      "当前学习率：0.001000\n",
      "EarlyStopping counter: 5 out of 36\n",
      "\n",
      "- Epoch: 7 - Train_loss: 0.71317 - Train_acc: 0.60009 - Val_loss: 0.81326 - Val_acc: 0.500000 - T_Time: 40.486\n",
      "当前学习率：0.001000\n",
      "EarlyStopping counter: 6 out of 36\n",
      "\n",
      "- Epoch: 8 - Train_loss: 0.71302 - Train_acc: 0.60025 - Val_loss: 0.75969 - Val_acc: 0.553571 - T_Time: 40.227\n",
      "当前学习率：0.001000\n",
      "EarlyStopping counter: 7 out of 36\n",
      "\n",
      "- Epoch: 9 - Train_loss: 0.71240 - Train_acc: 0.60086 - Val_loss: 0.74630 - Val_acc: 0.566964 - T_Time: 40.061\n",
      "当前学习率：0.001000\n",
      "EarlyStopping counter: 8 out of 36\n",
      "\n",
      "- Epoch: 10 - Train_loss: 0.71256 - Train_acc: 0.60071 - Val_loss: 0.75969 - Val_acc: 0.553571 - T_Time: 40.110\n",
      "当前学习率：0.001000\n",
      "EarlyStopping counter: 9 out of 36\n",
      "\n",
      "- Epoch: 11 - Train_loss: 0.68290 - Train_acc: 0.62480 - Val_loss: 0.78279 - Val_acc: 0.526786 - T_Time: 40.856\n",
      "当前学习率：0.001000\n",
      "EarlyStopping counter: 10 out of 36\n",
      "\n",
      "- Epoch: 12 - Train_loss: 0.59879 - Train_acc: 0.70059 - Val_loss: 0.64499 - Val_acc: 0.665179 - T_Time: 40.797\n",
      "当前学习率：0.000100\n",
      "Find better model in Epoch 12, saving model.\n",
      "Validation loss decreased (0.712720 --> 0.644990).  Saving model ...\n",
      "                    --------------------------------------------------\n",
      "\n",
      "- Epoch: 13 - Train_loss: 0.59300 - Train_acc: 0.70070 - Val_loss: 0.65699 - Val_acc: 0.638393 - T_Time: 40.584\n",
      "当前学习率：0.000100\n",
      "EarlyStopping counter: 1 out of 36\n",
      "\n",
      "- Epoch: 14 - Train_loss: 0.57854 - Train_acc: 0.71716 - Val_loss: 0.72344 - Val_acc: 0.562500 - T_Time: 40.476\n",
      "当前学习率：0.000100\n",
      "EarlyStopping counter: 2 out of 36\n",
      "\n",
      "- Epoch: 15 - Train_loss: 0.57525 - Train_acc: 0.72172 - Val_loss: 0.65421 - Val_acc: 0.593750 - T_Time: 40.453\n",
      "当前学习率：0.000100\n",
      "EarlyStopping counter: 3 out of 36\n",
      "\n",
      "- Epoch: 16 - Train_loss: 0.57175 - Train_acc: 0.72698 - Val_loss: 0.68979 - Val_acc: 0.616071 - T_Time: 40.334\n",
      "当前学习率：0.000100\n",
      "EarlyStopping counter: 4 out of 36\n",
      "\n",
      "- Epoch: 17 - Train_loss: 0.57262 - Train_acc: 0.73046 - Val_loss: 0.65549 - Val_acc: 0.625000 - T_Time: 40.490\n",
      "当前学习率：0.000100\n",
      "EarlyStopping counter: 5 out of 36\n",
      "\n",
      "- Epoch: 18 - Train_loss: 0.57805 - Train_acc: 0.71834 - Val_loss: 0.71001 - Val_acc: 0.566964 - T_Time: 40.584\n",
      "当前学习率：0.000100\n",
      "EarlyStopping counter: 6 out of 36\n",
      "\n",
      "- Epoch: 19 - Train_loss: 0.57088 - Train_acc: 0.73143 - Val_loss: 0.66762 - Val_acc: 0.620536 - T_Time: 40.324\n",
      "当前学习率：0.000100\n",
      "EarlyStopping counter: 7 out of 36\n",
      "\n",
      "- Epoch: 20 - Train_loss: 0.55492 - Train_acc: 0.74790 - Val_loss: 0.63750 - Val_acc: 0.647321 - T_Time: 40.347\n",
      "当前学习率：0.000100\n",
      "Find better model in Epoch 20, saving model.\n",
      "Validation loss decreased (0.644990 --> 0.637503).  Saving model ...\n",
      "                    --------------------------------------------------\n",
      "\n",
      "- Epoch: 21 - Train_loss: 0.55865 - Train_acc: 0.73946 - Val_loss: 0.67392 - Val_acc: 0.625000 - T_Time: 40.163\n",
      "当前学习率：0.000100\n",
      "EarlyStopping counter: 1 out of 36\n",
      "\n",
      "- Epoch: 22 - Train_loss: 0.54542 - Train_acc: 0.76176 - Val_loss: 0.64379 - Val_acc: 0.656250 - T_Time: 40.169\n",
      "当前学习率：0.000100\n",
      "EarlyStopping counter: 2 out of 36\n",
      "\n",
      "- Epoch: 23 - Train_loss: 0.55135 - Train_acc: 0.74693 - Val_loss: 0.62350 - Val_acc: 0.674107 - T_Time: 40.193\n",
      "当前学习率：0.000100\n",
      "Find better model in Epoch 23, saving model.\n",
      "Validation loss decreased (0.637503 --> 0.623496).  Saving model ...\n",
      "                    --------------------------------------------------\n",
      "\n",
      "- Epoch: 24 - Train_loss: 0.53464 - Train_acc: 0.76785 - Val_loss: 0.64454 - Val_acc: 0.674107 - T_Time: 40.464\n",
      "当前学习率：0.000100\n",
      "EarlyStopping counter: 1 out of 36\n",
      "\n",
      "- Epoch: 25 - Train_loss: 0.53111 - Train_acc: 0.77552 - Val_loss: 0.65717 - Val_acc: 0.616071 - T_Time: 40.378\n",
      "当前学习率：0.000010\n",
      "EarlyStopping counter: 2 out of 36\n",
      "\n",
      "- Epoch: 26 - Train_loss: 0.52512 - Train_acc: 0.78478 - Val_loss: 0.65442 - Val_acc: 0.633929 - T_Time: 39.911\n",
      "当前学习率：0.000010\n",
      "EarlyStopping counter: 3 out of 36\n",
      "\n",
      "- Epoch: 27 - Train_loss: 0.51520 - Train_acc: 0.79117 - Val_loss: 0.67872 - Val_acc: 0.620536 - T_Time: 40.251\n",
      "当前学习率：0.000010\n",
      "EarlyStopping counter: 4 out of 36\n",
      "\n",
      "- Epoch: 28 - Train_loss: 0.50740 - Train_acc: 0.79808 - Val_loss: 0.66574 - Val_acc: 0.625000 - T_Time: 40.369\n",
      "当前学习率：0.000010\n",
      "EarlyStopping counter: 5 out of 36\n",
      "\n",
      "- Epoch: 29 - Train_loss: 0.50588 - Train_acc: 0.80089 - Val_loss: 0.66830 - Val_acc: 0.611607 - T_Time: 40.387\n",
      "当前学习率：0.000010\n",
      "EarlyStopping counter: 6 out of 36\n",
      "\n",
      "- Epoch: 30 - Train_loss: 0.51511 - Train_acc: 0.79133 - Val_loss: 0.68179 - Val_acc: 0.593750 - T_Time: 40.387\n",
      "当前学习率：0.000010\n",
      "EarlyStopping counter: 7 out of 36\n",
      "\n",
      "- Epoch: 31 - Train_loss: 0.50020 - Train_acc: 0.81291 - Val_loss: 0.69424 - Val_acc: 0.584821 - T_Time: 40.523\n",
      "当前学习率：0.000010\n",
      "EarlyStopping counter: 8 out of 36\n",
      "\n",
      "- Epoch: 32 - Train_loss: 0.49908 - Train_acc: 0.81224 - Val_loss: 0.70545 - Val_acc: 0.584821 - T_Time: 40.386\n",
      "当前学习率：0.000010\n",
      "EarlyStopping counter: 9 out of 36\n",
      "\n",
      "- Epoch: 33 - Train_loss: 0.49886 - Train_acc: 0.81352 - Val_loss: 0.68314 - Val_acc: 0.611607 - T_Time: 40.238\n",
      "当前学习率：0.000010\n",
      "EarlyStopping counter: 10 out of 36\n",
      "\n",
      "- Epoch: 34 - Train_loss: 0.50211 - Train_acc: 0.81010 - Val_loss: 0.67963 - Val_acc: 0.611607 - T_Time: 40.430\n",
      "当前学习率：0.000010\n",
      "EarlyStopping counter: 11 out of 36\n",
      "\n",
      "- Epoch: 35 - Train_loss: 0.48252 - Train_acc: 0.83475 - Val_loss: 0.69256 - Val_acc: 0.602679 - T_Time: 40.603\n",
      "当前学习率：0.000001\n",
      "EarlyStopping counter: 12 out of 36\n",
      "\n",
      "- Epoch: 36 - Train_loss: 0.48707 - Train_acc: 0.82651 - Val_loss: 0.70466 - Val_acc: 0.602679 - T_Time: 40.245\n",
      "当前学习率：0.000001\n",
      "EarlyStopping counter: 13 out of 36\n",
      "\n",
      "- Epoch: 37 - Train_loss: 0.48294 - Train_acc: 0.82784 - Val_loss: 0.66788 - Val_acc: 0.625000 - T_Time: 40.617\n",
      "当前学习率：0.000001\n",
      "EarlyStopping counter: 14 out of 36\n",
      "\n",
      "- Epoch: 38 - Train_loss: 0.48041 - Train_acc: 0.83367 - Val_loss: 0.65706 - Val_acc: 0.633929 - T_Time: 40.293\n",
      "当前学习率：0.000001\n",
      "EarlyStopping counter: 15 out of 36\n",
      "\n",
      "- Epoch: 39 - Train_loss: 0.48222 - Train_acc: 0.83071 - Val_loss: 0.68741 - Val_acc: 0.611607 - T_Time: 40.310\n",
      "当前学习率：0.000001\n",
      "EarlyStopping counter: 16 out of 36\n",
      "\n",
      "- Epoch: 40 - Train_loss: 0.48184 - Train_acc: 0.82769 - Val_loss: 0.71096 - Val_acc: 0.571429 - T_Time: 39.946\n",
      "当前学习率：0.000001\n",
      "EarlyStopping counter: 17 out of 36\n",
      "\n",
      "- Epoch: 41 - Train_loss: 0.48486 - Train_acc: 0.82431 - Val_loss: 0.69008 - Val_acc: 0.602679 - T_Time: 39.946\n",
      "当前学习率：0.000001\n",
      "EarlyStopping counter: 18 out of 36\n",
      "\n",
      "- Epoch: 42 - Train_loss: 0.48770 - Train_acc: 0.82319 - Val_loss: 0.69995 - Val_acc: 0.593750 - T_Time: 40.063\n",
      "当前学习率：0.000001\n",
      "EarlyStopping counter: 19 out of 36\n",
      "\n",
      "- Epoch: 43 - Train_loss: 0.48277 - Train_acc: 0.82503 - Val_loss: 0.67801 - Val_acc: 0.620536 - T_Time: 39.994\n",
      "当前学习率：0.000001\n",
      "EarlyStopping counter: 20 out of 36\n",
      "\n",
      "- Epoch: 44 - Train_loss: 0.48067 - Train_acc: 0.83204 - Val_loss: 0.68483 - Val_acc: 0.602679 - T_Time: 40.138\n",
      "当前学习率：0.000001\n",
      "EarlyStopping counter: 21 out of 36\n",
      "\n",
      "- Epoch: 45 - Train_loss: 0.48633 - Train_acc: 0.82718 - Val_loss: 0.66560 - Val_acc: 0.633929 - T_Time: 40.257\n",
      "当前学习率：0.000000\n",
      "EarlyStopping counter: 22 out of 36\n",
      "\n",
      "- Epoch: 46 - Train_loss: 0.47473 - Train_acc: 0.83679 - Val_loss: 0.69299 - Val_acc: 0.602679 - T_Time: 40.575\n",
      "当前学习率：0.000000\n",
      "EarlyStopping counter: 23 out of 36\n",
      "\n",
      "- Epoch: 47 - Train_loss: 0.47613 - Train_acc: 0.83700 - Val_loss: 0.68345 - Val_acc: 0.602679 - T_Time: 40.041\n",
      "当前学习率：0.000000\n",
      "EarlyStopping counter: 24 out of 36\n",
      "\n",
      "- Epoch: 48 - Train_loss: 0.48158 - Train_acc: 0.82687 - Val_loss: 0.70709 - Val_acc: 0.593750 - T_Time: 40.299\n",
      "当前学习率：0.000000\n",
      "EarlyStopping counter: 25 out of 36\n",
      "\n",
      "- Epoch: 49 - Train_loss: 0.48328 - Train_acc: 0.82503 - Val_loss: 0.67733 - Val_acc: 0.598214 - T_Time: 40.333\n",
      "当前学习率：0.000000\n",
      "EarlyStopping counter: 26 out of 36\n",
      "\n",
      "- Epoch: 50 - Train_loss: 0.48493 - Train_acc: 0.82651 - Val_loss: 0.68260 - Val_acc: 0.620536 - T_Time: 40.036\n",
      "当前学习率：0.000000\n",
      "EarlyStopping counter: 27 out of 36\n",
      "\n",
      "- Epoch: 51 - Train_loss: 0.48021 - Train_acc: 0.83398 - Val_loss: 0.67024 - Val_acc: 0.620536 - T_Time: 40.084\n",
      "当前学习率：0.000000\n",
      "EarlyStopping counter: 28 out of 36\n",
      "\n",
      "- Epoch: 52 - Train_loss: 0.48154 - Train_acc: 0.82984 - Val_loss: 0.66888 - Val_acc: 0.633929 - T_Time: 40.285\n",
      "当前学习率：0.000000\n",
      "EarlyStopping counter: 29 out of 36\n",
      "\n",
      "- Epoch: 53 - Train_loss: 0.48149 - Train_acc: 0.83066 - Val_loss: 0.71692 - Val_acc: 0.580357 - T_Time: 40.050\n",
      "当前学习率：0.000000\n",
      "EarlyStopping counter: 30 out of 36\n",
      "\n",
      "- Epoch: 54 - Train_loss: 0.48470 - Train_acc: 0.82488 - Val_loss: 0.67011 - Val_acc: 0.620536 - T_Time: 40.142\n",
      "当前学习率：0.000000\n",
      "EarlyStopping counter: 31 out of 36\n",
      "\n",
      "- Epoch: 55 - Train_loss: 0.47788 - Train_acc: 0.83582 - Val_loss: 0.65734 - Val_acc: 0.629464 - T_Time: 40.054\n",
      "当前学习率：0.000000\n",
      "EarlyStopping counter: 32 out of 36\n",
      "\n",
      "- Epoch: 56 - Train_loss: 0.47971 - Train_acc: 0.83132 - Val_loss: 0.70296 - Val_acc: 0.593750 - T_Time: 39.967\n",
      "当前学习率：0.000000\n",
      "EarlyStopping counter: 33 out of 36\n",
      "\n",
      "- Epoch: 57 - Train_loss: 0.48701 - Train_acc: 0.82324 - Val_loss: 0.67326 - Val_acc: 0.620536 - T_Time: 40.382\n",
      "当前学习率：0.000000\n",
      "EarlyStopping counter: 34 out of 36\n",
      "\n",
      "- Epoch: 58 - Train_loss: 0.48042 - Train_acc: 0.83101 - Val_loss: 0.68595 - Val_acc: 0.611607 - T_Time: 40.582\n",
      "当前学习率：0.000000\n",
      "EarlyStopping counter: 35 out of 36\n",
      "\n",
      "- Epoch: 59 - Train_loss: 0.48191 - Train_acc: 0.83204 - Val_loss: 0.69523 - Val_acc: 0.602679 - T_Time: 40.024\n",
      "当前学习率：0.000000\n",
      "EarlyStopping counter: 36 out of 36\n",
      "\n",
      "Early stopping\n",
      "Fold 1 Training Finished\n",
      "Training Finished\n"
     ]
    }
   ],
   "source": [
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "\n",
    "for fold in range(FOLDS):\n",
    "    \n",
    "    early_stopping = EarlyStopping(PATIENCE, verbose=True, model_path=model_path, delta=0)\n",
    "    #train_dataset,valid_dataset = get_k_fold_dataset(fold=int(fold+1),x = train_x,y=train_y,k=FOLDS,random_seed = random_seed)\n",
    "    train_dataloader = Data.DataLoader(dataset=train_data, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    valid_dataloader = Data.DataLoader(dataset=validate_date, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "    NET[fold].to(DEVICE)\n",
    "    optimizer  = torch.optim.Adam(NET[fold].parameters(), lr=LR,weight_decay=1e-4)  \n",
    "    criterion = torch.nn.CrossEntropyLoss()   \n",
    "    #等间隔调整学习率\n",
    "    scheduler =torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1, last_epoch=-1)\n",
    "    #scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer,T_max = 32)\n",
    "    best_loss = 3\n",
    "    for epoch in range(1,EPOCHS):\n",
    "        time_all=0\n",
    "        start_time = time.time()\n",
    "        train_loss,train_acc = train_model(train_dataloader, NET[fold], criterion, optimizer,DEVICE) # 训练模型\n",
    "        \n",
    "\n",
    "        time_all = time.time()-start_time\n",
    "        validate_loss,validate_acc = test_model(valid_dataloader,criterion,NET[fold],DEVICE) # 测试模型\n",
    "        writer.add_scalars(main_tag=str(fold)+'_Loss',tag_scalar_dict={'train': train_loss,'validate': validate_loss},global_step=epoch)\n",
    "        writer.add_scalars(main_tag=str(fold)+'_Accuracy',tag_scalar_dict={'train': train_acc,'validate': validate_acc},global_step=epoch)\n",
    "        writer.add_scalars(main_tag=str(fold)+'_LearningRate',tag_scalar_dict={'LR': optimizer.state_dict()['param_groups'][0]['lr']},global_step=epoch)\n",
    "\n",
    "        print('- Epoch: %d - Train_loss: %.5f - Train_acc: %.5f - Val_loss: %.5f - Val_acc: %5f - T_Time: %.3f' %(epoch,train_loss,train_acc,validate_loss,validate_acc,time_all))\n",
    "        print('当前学习率：%f' %optimizer.state_dict()['param_groups'][0]['lr'])\n",
    "\n",
    "        if validate_loss < best_loss:\n",
    "            best_loss = validate_loss\n",
    "            print('Find better model in Epoch {0}, saving model.'.format(epoch))\n",
    "            #torch.save(NET[fold],  model_path+'/best_model_' + str(fold) + '.pt')  # 保存最优模型\n",
    "        else:\n",
    "            scheduler.step() # 学习率迭代\n",
    "        #是否满足早停法条件\n",
    "        if(early_stopping(validate_loss,NET[fold])):\n",
    "            print(\"Early stopping\")\n",
    "            break\n",
    "\n",
    "    print('Fold %d Training Finished' %(fold+1))\n",
    "    torch.cuda.empty_cache()# 清空显卡cuda\n",
    "print('Training Finished')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.13 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
