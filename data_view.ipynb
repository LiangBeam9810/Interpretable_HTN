{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 1\n",
    "%aimport ECGDataset\n",
    "%aimport Models\n",
    "%aimport train_test_validat\n",
    "%aimport self_attention\n",
    "%aimport ECGplot\n",
    "%aimport Net\n",
    "\n",
    "import ECGDataset \n",
    "import Models \n",
    "import Net\n",
    "from train_test_validat import *\n",
    "from self_attention import *\n",
    "import matplotlib.pyplot as plt\n",
    "import ecg_plot\n",
    "import cam\n",
    "import ECGplot\n",
    "import ECGHandle\n",
    "import torch\n",
    "import torch.utils.data as Data\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import random\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "import time\n",
    "import math\n",
    "import os\n",
    "import gc\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "\n",
    "def seed_torch(seed=2023):\n",
    "\trandom.seed(seed)\n",
    "\tos.environ['PYTHONHASHSEED'] = str(seed) # 为了禁止hash随机化，使得实验可复现\n",
    "\tnp.random.seed(seed)\n",
    "\ttorch.manual_seed(seed)\n",
    "\ttorch.cuda.manual_seed(seed)\n",
    "\ttorch.cuda.manual_seed_all(seed) # if you are using multi-GPU.\n",
    "\ttorch.backends.cudnn.benchmark = False \n",
    "\ttorch.backends.cudnn.deterministic = True\n",
    "    # torch.backends.cudnn.enabled = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "EcgChannles_num = 12\n",
    "EcgLength_num = 5000\n",
    "DEVICE = \"cpu\"\n",
    "seed_torch(2023)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "supplement_diagnose = pd.read_csv('./补充诊断.csv',encoding='utf-8-sig')\n",
    "\n",
    "# 使用groupby方法按照ID分组，然后使用agg方法将data列拼接在一起\n",
    "supplement_diagnose = supplement_diagnose.groupby('ID')['住院所有诊断'].agg(lambda x: ','.join(x.astype(str))).reset_index()\n",
    "# 使用str.contains方法筛选data列中包含‘高血压’的行\n",
    "filtered_df = supplement_diagnose[supplement_diagnose['住院所有诊断'].str.contains('高血压')]\n",
    "# 将筛选后的ID列转换为list\n",
    "id_list = filtered_df['ID'].astype(str).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "            orginal   removed diagnose NaN\n",
      "   nums      200082          199997       \n",
      "              HTN             NHTN        \n",
      "   nums       3273           196724       \n",
      "\n",
      "\n",
      "            orginal      removed ID NaN   \n",
      "   nums      199997          199995       \n",
      "              HTN             NHTN        \n",
      "   nums       3273           196722       \n",
      "\n",
      "\n",
      "            orginal      filtered ages    \n",
      "   nums      199995          183068       \n",
      "              HTN             NHTN        \n",
      "   nums       3220           179848       \n",
      "\n",
      "\n",
      "            orginal            QC         \n",
      "   nums      183068          69819        \n",
      "              HTN             NHTN        \n",
      "   nums       1477           68342        \n",
      "\n",
      "\n",
      "     reset num:      18448\n",
      "  ERR labels num:     24  \n",
      "            orginal      correct label    \n",
      "   nums      69819           69819        \n",
      "              HTN             NHTN        \n",
      "   nums      18690           51129        \n",
      "\n",
      "\n",
      "   ERR ages num:     15189\n",
      "            orginal       correct age     \n",
      "   nums      69819           69819        \n",
      "              HTN             NHTN        \n",
      "   nums      18690           51129        \n",
      "\n",
      "\n",
      "            orginal    remove diagnose起搏  \n",
      "   nums      69819           69197        \n",
      "              HTN             NHTN        \n",
      "   nums      18336           50861        \n",
      "           remove HTN     remove NHTN     \n",
      "   nums       354             268         \n",
      "\n",
      "\n",
      "            orginal    remove diagnose房颤  \n",
      "   nums      69197           66059        \n",
      "              HTN             NHTN        \n",
      "   nums      16802           49257        \n",
      "           remove HTN     remove NHTN     \n",
      "   nums       1534            1604        \n",
      "\n",
      "\n",
      "            orginal   remove diagnose左束支传导阻滞\n",
      "   nums      66059           66059        \n",
      "              HTN             NHTN        \n",
      "   nums      16802           49257        \n",
      "           remove HTN     remove NHTN     \n",
      "   nums        0               0          \n",
      "\n",
      "\n",
      "            orginal   remove diagnose左前分支阻滞\n",
      "   nums      66059           65248        \n",
      "              HTN             NHTN        \n",
      "   nums      16478           48770        \n",
      "           remove HTN     remove NHTN     \n",
      "   nums       324             487         \n"
     ]
    }
   ],
   "source": [
    "data_root = '/workspace/data/Preprocess_HTN/datas_/'\n",
    "ALL_data = pd.read_csv(data_root+'/All_data_handled_ID_range_age_IDimputate.csv',low_memory=False)\n",
    "ALL_data = ECGHandle.change_label(ALL_data)\n",
    "ALL_data = ECGHandle.filter_ID(ALL_data)\n",
    "# ALL_data = ECGHandle.filter_departmentORlabel(ALL_data,'外科')\n",
    "ALL_data = ECGHandle.filter_ages(ALL_data,18) \n",
    "ALL_data = ECGHandle.filter_QC(ALL_data)\n",
    "ALL_data = ECGHandle.correct_label(ALL_data,reset_list = id_list)\n",
    "ALL_data = ECGHandle.correct_age(ALL_data)\n",
    "ALL_data = ECGHandle.filter_diagnose(ALL_data,'起搏')\n",
    "ALL_data = ECGHandle.filter_diagnose(ALL_data,'房颤')\n",
    "ALL_data = ECGHandle.filter_diagnose(ALL_data,'左束支传导阻滞')\n",
    "ALL_data = ECGHandle.filter_diagnose(ALL_data,'左前分支阻滞')\n",
    "ALL_data[ALL_data['label'] == 0]['住院号'].unique().__len__()\n",
    "ALL_data = ALL_data.rename(columns={'住院号':'ID','年龄':'age','性别':'gender','姓名':'name'}) \n",
    "ALL_data_buffer = ALL_data.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_torch(2023)# keep the the set the same\n",
    "ALL_data_buffer = ALL_data.copy()\n",
    "ALL_data_buffer = ALL_data_buffer.sample(frac=1).reset_index(drop=True) #打乱顺序\n",
    "####################################################################随机选取test\n",
    "# test_df,tv_df = Pair_ID(ALL_data_buffer,0.2,Range_max=15,pair_num=1)\n",
    "# ####################################################################随机选取test\n",
    "test_df,tv_df = Pair_ID(ALL_data,0.2,Range_max=15,pair_num=1)\n",
    "test_dataset = ECGHandle.ECG_Dataset(data_root,test_df,preprocess = True)\n",
    "\n",
    "# test_df =all_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Models_path = '/workspace/data/Interpretable_HTN/model/20230320_011459/20230320_061146/BestF1_2.pt'\n",
    "save_root = Models_path[:-3]+'/'    \n",
    "layervalue_root = save_root+'/layervalue/'    \n",
    "NET = [Net.MLBFNet_GUR_o(True,True,True,2,Dropout_rate=0.3), ] # type: ignore    \n",
    "criterion = torch.nn.CrossEntropyLoss()    \n",
    "testmodel = NET[0].to(DEVICE)    \n",
    "testmodel.load_state_dict(torch.load(Models_path))    \n",
    "test_dataloader = Data.DataLoader(dataset=test_dataset, batch_size=1, shuffle=False)      \n",
    "y_true,y_pred,y_out,test_loss,test_acc = eval_model(test_dataloader,criterion,testmodel,DEVICE) # 验证模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset.infos['out'] = np.array(y_out)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_double_list = ['748939',\n",
    "'764590',\n",
    "'763667',\n",
    "'579114',\n",
    "'753320',\n",
    "'795543',\n",
    "'788055',\n",
    "'757960',\n",
    "'772546',\n",
    "'786546',\n",
    "'780224',\n",
    "'813202',\n",
    "'814775',\n",
    "'544375',\n",
    "'828347',\n",
    "'840799',\n",
    "'757558',\n",
    "'803369',\n",
    "'822443',\n",
    "'801013',\n",
    "'327715',\n",
    "'812001',\n",
    "'378927',\n",
    "'839205',\n",
    "'824053',\n",
    "'809810',\n",
    "'633009',\n",
    "'826823',\n",
    "'825537',\n",
    "'794560',]\n",
    "\n",
    "p_double_df = test_dataset.infos[[True if i in p_double_list else False for i in test_dataset.infos['ID']]]\n",
    "p_double_df.to_csv('p_double_df.csv',encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_area(ECG:np.array,start_index:np.array,end_index:np.array): # type: ignore \n",
    "    p_area = 0\n",
    "    for lead in range(12):\n",
    "        for bet in range(len(start_index)):\n",
    "            start = int(start_index[bet])\n",
    "            end = int(end_index[bet])\n",
    "            p_area = p_area + abs((ECG[lead,start:end] - ECG[lead,start:end].min()).sum())/(end-start)\n",
    "    p_area = p_area/((len(start_index)))\n",
    "    return p_area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "waves_location_file_root = '/workspace/data/Preprocess_HTN/datas_/Wave/'\n",
    "lead_index = ['I', 'II', 'III', 'aVR', 'aVL', 'aVF', 'V1', 'V2', 'V3', 'V4', 'V5', 'V6']\n",
    "Amplitude_name_list = []\n",
    "for i in range(12):\n",
    "    Amplitude_name_list.append('P_Amplitude_'+lead_index[i])\n",
    "    Amplitude_name_list.append('Q_Amplitude_'+lead_index[i])\n",
    "    Amplitude_name_list.append('R_Amplitude_'+lead_index[i])\n",
    "    Amplitude_name_list.append('S_Amplitude_'+lead_index[i])\n",
    "    Amplitude_name_list.append('T_Amplitude_'+lead_index[i])\n",
    "Amplitude_features_12leads =    pd.DataFrame(columns=Amplitude_name_list)\n",
    "Timing_features = pd.DataFrame(columns=['P_wave_duration', 'QRS_duration', 'T_wave_duration','PQ_interval', 'PR_interval', 'QT_interval', 'QTc_interval', 'RR_interval','P_area'])\n",
    "infos = pd.DataFrame(columns=test_dataset.infos.columns)\n",
    "\n",
    "for index in range(test_dataset.__len__()):#test_dataset.__len__()\n",
    "    try:\n",
    "        info =test_dataset.infos.iloc[index]\n",
    "        ECG,_ = test_dataset.__getitem__(index)\n",
    "        ECGfile_name = info['ECGFilename']\n",
    "        \n",
    "        FPT = np.load(waves_location_file_root+'/FPT/'+ECGfile_name+'.npy')\n",
    "        Timing_feature_sync = np.load(waves_location_file_root+'/Timing_feature_sync/'+ECGfile_name+'.npy')\n",
    "        Amplitude_feature_12leads = np.load(waves_location_file_root+'/Amplitude_feature_12leads/'+ECGfile_name+'.npy')\n",
    "        \n",
    "        Amplitude_feature_12leads = pd.DataFrame((Amplitude_feature_12leads.transpose(1,0,2)[1:-1]).reshape(-1,60),columns=Amplitude_name_list) #lead,bet,5 -> bet,lead,5 ->bet,lead*5 \n",
    "        FPT = pd.DataFrame(FPT[1:-1],columns=['P_start', 'P_peak', 'P_end','Q_start', 'Q_peak', 'R_peak', 'S_peak', 'S_end','res0','T_start', 'T_peak', 'T_end','res0'])\n",
    "        Timing_feature_sync = pd.DataFrame(Timing_feature_sync[1:-1],columns=['P_wave_duration', 'QRS_duration', 'T_wave_duration','PQ_interval', 'PR_interval', 'QT_interval', 'QTc_interval', 'RR_interval'])\n",
    "        \n",
    "        Timing_feature = Timing_feature_sync \n",
    "        # Timing_feature = Timing_feature_sync.div(Timing_feature_sync['RR_interval'],axis=0) #(bet,13)把每个间隔除以每个RR间期\n",
    "        # Timing_feature['RR_interval'] = Timing_feature_sync['RR_interval']#恢复RR间期\n",
    "        Timing_feature = Timing_feature[1:-1].mean() #\n",
    "        Timing_feature['P_area'] = calculate_area(np.array(ECG),np.array(FPT['P_start']),np.array(FPT['P_end']))\n",
    "        \n",
    "        \n",
    "        Amplitude_feature = Amplitude_feature_12leads.mean()\n",
    "        Amplitude_features_12leads = Amplitude_features_12leads.append(Amplitude_feature,True)\n",
    "        Timing_features = Timing_features.append(Timing_feature,True )\n",
    "        \n",
    "        infos = infos.append(info)\n",
    "    except:\n",
    "        ECGfile_name = test_dataset.infos.iloc[index]['ECGFilename']\n",
    "        print('Err :',ECGfile_name)\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Timing_features['ECGFilename'] = infos['ECGFilename'].values\n",
    "Amplitude_features_12leads['ECGFilename'] = infos['ECGFilename'].values\n",
    "select_info = ['name','ID','gender','age','检查时间','临床诊断', '诊断','ECGFilename', 'xmlPath', 'Q', 'label', 'out']\n",
    "test_df_with_ECG_features = pd.merge(infos[select_info],pd.merge(Amplitude_features_12leads,Timing_features,on='ECGFilename',how='inner'),on='ECGFilename',how='inner')\n",
    "test_df_with_ECG_features.to_csv('test_2023_infos_and_features.csv')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 常规操作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ALL_data = ALL_data.rename(columns={'住院号':'ID','年龄':'age','性别':'gender','姓名':'name'}) \n",
    "ALL_data_buffer = ALL_data.copy()\n",
    "seed_torch(2023)\n",
    "ALL_data_buffer = ALL_data_buffer.sample(frac=1).reset_index(drop=True) #打乱顺序\n",
    "# all_dataset = ECGHandle.ECG_Dataset(data_root,ALL_data_buffer,preprocess = True)\n",
    "####################################################################随机选取test\n",
    "test_df,tv_df = Pair_ID(ALL_data,0.2,Range_max=15,pair_num=1)\n",
    "test_dataset = ECGHandle.ECG_Dataset(data_root,test_df,preprocess = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Models_path = '/workspace/data/Interpretable_HTN/model/20230322_030450/20230322_030450/BestF1_0.pt'\n",
    "save_root = Models_path[:-3]+'/'    \n",
    "layervalue_root = save_root+'/layervalue/'    \n",
    "NET = [Net.MLBFNet_GUR_o(True,True,True,2,Dropout_rate=0.3), ] # type: ignore    \n",
    "criterion = torch.nn.CrossEntropyLoss()    \n",
    "testmodel = NET[0].to(DEVICE)    \n",
    "testmodel.load_state_dict(torch.load(Models_path))    \n",
    "test_dataloader = Data.DataLoader(dataset=test_dataset, batch_size=1, shuffle=False)      \n",
    "y_true,y_pred,y_out,test_loss,test_acc = eval_model(test_dataloader,criterion,testmodel,DEVICE) # 验证模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(y_out).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df_save = test_df.copy()\n",
    "test_df_save['predict HTN possibility'] = np.array(y_out)[:,1]\n",
    "print(test_df_save.head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df_save[test_df_save['label']==1].__len__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df_save.to_csv('./test.csv',encoding='utf_8_sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ALL_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicated_HTN_df = ALL_data[ALL_data.duplicated(subset=['ID'], keep=False) & (ALL_data['label']==1)& ( ~ ALL_data['检查时间'].isnull()) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicated_HTN_df.__len__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicated_HTN_df['检查时间'] = pd.to_datetime(duplicated_HTN_df['检查时间'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicated_HTN_df['date_diff'] = duplicated_HTN_df.groupby('ID')['检查时间'].apply(lambda x:abs( x.diff()).dt.total_seconds())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicated_HTN_ID_list_buffer = list(duplicated_HTN_df[duplicated_HTN_df['date_diff']>(31536000/2)]['ID'])\n",
    "print(duplicated_HTN_ID_list_buffer.__len__())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicated_HTN_df= ALL_data[ALL_data['ID'].isin(duplicated_HTN_ID_list_buffer)].copy()\n",
    "duplicated_HTN_dataset = ECGHandle.ECG_Dataset(data_root,duplicated_HTN_df,preprocess = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Models_path = '/workspace/data/Interpretable_HTN/model/20230322_030450/20230322_030450/BestF1_0.pt'\n",
    "save_root = Models_path[:-3]+'/'    \n",
    "layervalue_root = save_root+'/layervalue/'    \n",
    "NET = [Net.MLBFNet_GUR_o(True,True,True,2,Dropout_rate=0.3), ] # type: ignore    \n",
    "criterion = torch.nn.CrossEntropyLoss()    \n",
    "testmodel = NET[0].to(DEVICE)    \n",
    "testmodel.load_state_dict(torch.load(Models_path))    \n",
    "duplicated_HTN_dataloader = Data.DataLoader(dataset=duplicated_HTN_dataset, batch_size=1, shuffle=False)      \n",
    "y_true,y_pred,y_out,test_loss,test_acc = eval_model(duplicated_HTN_dataloader,criterion,testmodel,DEVICE) # 验证模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicated_HTN_df['predict HTN possibility'] = np.array(y_out)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicated_HTN_df[duplicated_HTN_df['predict HTN possibility']<0.5]['ID'].unique().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicated_HTN_df.to_csv('./duplicated_HTN_间隔半年以上.csv',encoding='utf_8_sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jpg_path = './jpg/duplicated_HTN/'\n",
    "for index in range(duplicated_HTN_dataset.__len__()):#test_dataset.__len__()\n",
    "    info =duplicated_HTN_dataset.infos.iloc[index]\n",
    "    file_name = info['ID']+'_'+info['ECGFilename']\n",
    "    ID = info['ID']\n",
    "    date = info['检查时间']\n",
    "    age = info['age']\n",
    "    label = info['label']\n",
    "    ECG,labels = duplicated_HTN_dataset.__getitem__(index)\n",
    "    ECG = ECG*5000 #恢复\n",
    "    ecg_plot.plot(ECG*4.88/1000, sample_rate = 500, title = 'ID:'+str(ID)+' '+ 'label: '+ str(label) +' '+'Date: '+str(date)+' '+'age: '+str(age)  ,row_height= 10,show_grid=True,show_separate_line=True)\n",
    "    ecg_plot.save_as_jpg(file_name,jpg_path)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 测试集上分类错误的NHTN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ERR_NHTN_df = test_df_save[ (test_df_save['label']==0) & (test_df_save['predict HTN possibility']>=0.5) ]\n",
    "print(ERR_NHTN_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ERR_NHTN_df.to_csv('./ERR_NHTN.csv',encoding='utf_8_sig')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## std & mean check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FOLDS = 5\n",
    "seed_torch(2023)\n",
    "tv_df = tv_df.sample(frac=1).reset_index(drop=True) #打乱顺序"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for fold in range(FOLDS):\n",
    "    print(\"Fold \"+str(fold)+\" of \"+str(FOLDS) + ' :')\n",
    "    tv_df_buffer = tv_df.copy()\n",
    "    HTN_tv_df = tv_df[(tv_df['label']==1) ].copy()\n",
    "    NHTN_tv_df = tv_df[(tv_df['label']==0) ].copy()\n",
    "    HTN_ID_tv_list = HTN_tv_df['ID'].unique().tolist() #tvset中所有的HTN的ID号\n",
    "    HTN_tv_size = HTN_tv_df['ID'].unique().__len__()\n",
    "    HTN_validate_size = int(HTN_tv_size//FOLDS)\n",
    "    validate_start_index = HTN_validate_size*fold #star index for validate\n",
    "    validate_df,tarin_df = Pair_ID(tv_df_buffer,0.2,star_index=validate_start_index,Range_max=15,pair_num=1)\n",
    "    validate_dataset = ECGHandle.ECG_Dataset(data_root,validate_df,preprocess = True)\n",
    "    \n",
    "    train_pair_df,_ = Pair_ID(tarin_df,1,star_index=0,Range_max=15,pair_num=1,shuffle=True)\n",
    "    train_dataset = ECGHandle.ECG_Dataset(data_root,train_pair_df ,preprocess = True)\n",
    "    for i in range(12):\n",
    "        print('lead:' ,i)\n",
    "        print('test:{}',test_dataset.datas[:,i,:].std(),test_dataset.datas[:,i,:].mean())\n",
    "        print('train:{}',train_dataset.datas[:,i,:].std(),train_dataset.datas[:,i,:].mean())\n",
    "        print('validat:{}',validate_dataset.datas[:,i,:].std(),validate_dataset.datas[:,i,:].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df[test_df['label'] == 0]['ID'].__len__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tv_df[tv_df['label'] == 0]['ID'].__len__()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 查看训练集、测试集、验证集的年龄、性别分布是否有差别"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ALL_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ALL_data.hist(column='age', by='gender')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.hist(column='age', by='gender')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_torch(2023)\n",
    "test_df,tv_df = Pair_ID(ALL_data,0.2,Range_max=15,pair_num=1)\n",
    "FOLDS = 5\n",
    "seed_torch(2020)\n",
    "tv_df = tv_df.sample(frac=1).reset_index(drop=True) #打乱顺序\n",
    "for fold in range(FOLDS):\n",
    "    print(\"Fold \"+str(fold)+\" of \"+str(FOLDS) + ' :')\n",
    "    tv_df_buffer = tv_df.copy()\n",
    "    HTN_tv_df = tv_df[(tv_df['label']==1) ].copy()\n",
    "    NHTN_tv_df = tv_df[(tv_df['label']==0) ].copy()\n",
    "    HTN_ID_tv_list = HTN_tv_df['ID'].unique().tolist() #tvset中所有的HTN的ID号\n",
    "    HTN_tv_size = HTN_tv_df['ID'].unique().__len__()\n",
    "    HTN_validate_size = int(HTN_tv_size//FOLDS)\n",
    "    validate_start_index = HTN_validate_size*fold #star index for validate\n",
    "    validate_df,tarin_df = Pair_ID(tv_df_buffer,0.2,star_index=validate_start_index,Range_max=15,pair_num=1)\n",
    "    \n",
    "    train_pair_df,_ = Pair_ID(tarin_df,1,star_index=0,Range_max=15,pair_num=1,shuffle=True)\n",
    "    validate_df.hist(column='age', by='gender')\n",
    "    tarin_df.hist(column='age', by='gender')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 按照[18, 30, 40, 50, 60, 70, 110]年龄分组"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 按照指定的区间进行分组\n",
    "bins = [18, 40, 60, 80, 110]\n",
    "labels = ['18-40', '41-60', '61-80', '81-110']\n",
    "ALL_data['agegroup'] = pd.cut(ALL_data['age'], bins=bins, labels=labels)\n",
    "ALL_data.hist('agegroup',by = 'label',sharex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 按照指定的区间进行分组\n",
    "bins = [18, 30, 40, 50, 60, 70, 80,110]\n",
    "labels = ['18-30', '31-40', '41-50', '51-60', '61-70', '71-80','81-110']\n",
    "ALL_data['agegroup'] = pd.cut(ALL_data['age'], bins=bins, labels=labels)\n",
    "ALL_data.hist(column='agegroup',by = 'label',sharex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HTN_df = ALL_data[(ALL_data['label']==1) ].drop_duplicates(subset=['ID'],keep = 'first').copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "# 按照agerange和gender进行分层抽样\n",
    "\n",
    "NHTN_df = ALL_data[(ALL_data['label']==0) ].drop_duplicates(subset=['ID'],keep = 'first').copy()\n",
    "HTN_df = ALL_data[(ALL_data['label']==1) ].drop_duplicates(subset=['ID'],keep = 'first').copy()\n",
    "TV_HTN_df, test_HTN_df = train_test_split(HTN_df, test_size=0.2, stratify=HTN_df[['agegroup', 'gender']])\n",
    "test_ID_list = pair_HTN(test_HTN_df.drop_duplicates(['ID'],keep='first'),NHTN_df.drop_duplicates(['ID'],keep='first'),\n",
    "                            Range_max=2,\n",
    "                            pair_num=1,\n",
    "                            shuffle=True)['ID'].tolist()#按照年龄和性别对每个ID号去配对 (先去除重复ID)\n",
    "pair_index = ALL_data[[True if i in test_ID_list else False for i in ALL_data['ID']]].index\n",
    "test_df = ALL_data.loc[pair_index].copy()\n",
    "left_index = ALL_data[[False if i in test_ID_list else True for i in ALL_data['ID']]].index #不在test_ID_list的ID 即为tv的\n",
    "TV_df = ALL_data.loc[left_index].copy()\n",
    "\n",
    "\n",
    "\n",
    "TV_NHTN_df = TV_df[(TV_df['label']==0) ].drop_duplicates(subset=['ID'],keep = 'first').copy()\n",
    "TV_HTN_df = TV_df[(TV_df['label']==1) ].drop_duplicates(subset=['ID'],keep = 'first').copy()\n",
    "fold_len= float((TV_HTN_df.__len__())//5) #每一fold的HTN的长度\n",
    "TV_HTN_buffer = TV_HTN_df.copy()\n",
    "\n",
    "validat_HTN_df_subsets = []\n",
    "for i in range(4):\n",
    "    TV_HTN_buffer, subset = train_test_split(TV_HTN_buffer, test_size=fold_len/(TV_HTN_buffer.__len__()), stratify=TV_HTN_buffer[['agegroup', 'gender']])\n",
    "    print(subset.__len__())\n",
    "    validat_HTN_df_subsets.append(subset)\n",
    "print(TV_HTN_buffer.__len__())\n",
    "validat_HTN_df_subsets.append(TV_HTN_buffer)\n",
    "\n",
    "validat_ID_list_subsets = []\n",
    "for i in range(validat_HTN_df_subsets.__len__()):\n",
    "    i_ID_list = pair_HTN(validat_HTN_df_subsets[i].drop_duplicates(['ID'],keep='first'),\n",
    "                         TV_NHTN_df.drop_duplicates(['ID'],keep='first'),\n",
    "                            Range_max=2,\n",
    "                            pair_num=1,\n",
    "                            shuffle=True)['ID'].tolist()#按照年龄和性别对每个ID号去配对 (先去除重复ID)\n",
    "    validat_ID_list_subsets.append(i_ID_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for fold in range(5):\n",
    "    print(\" \"*10+ \"Fold \"+str(fold)+\" of \"+str(5) + ' :')\n",
    "\n",
    "    pair_index = TV_df[[True if i in validat_ID_list_subsets[fold] else False for i in TV_df['ID']]].index\n",
    "    validate_df = TV_df.loc[pair_index].copy()\n",
    "    left_index = TV_df[[False if i in validat_ID_list_subsets[fold] else True for i in TV_df['ID']]].index #不在test_ID_list的ID 即为tv的\n",
    "    train_df = TV_df.loc[left_index].copy()\n",
    "    train_pair_df,_ = Pair_ID(train_df,1,star_index=0,Range_max=5,pair_num=1,shuffle=True)\n",
    "\n",
    "    validate_df.hist('age',by = 'label',sharex=True)\n",
    "    train_pair_df.hist('age',by = 'label',sharex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.hist('age',by = 'label',sharex=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jpg_path = './jpg/'\n",
    "for index in range(test_dataset.__len__()):#test_dataset.__len__()\n",
    "    info =test_dataset.infos.iloc[index]\n",
    "    file_name = info['ECGFilename']\n",
    "    ID = info['ID']\n",
    "    date = info['检查时间']\n",
    "    age = info['age']\n",
    "    label = info['label']\n",
    "    ECG,labels = test_dataset.__getitem__(index)\n",
    "    ECG = ECG*5000 #恢复\n",
    "    ecg_plot.plot(ECG*4.88/1000, sample_rate = 500, title = 'ID:'+str(ID)+' '+ 'label: '+ str(label) +' '+'Date: '+str(date)+' '+'age: '+str(age)  ,row_height= 10,show_grid=True,show_separate_line=True)\n",
    "    ecg_plot.save_as_jpg(file_name,jpg_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
