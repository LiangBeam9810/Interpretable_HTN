{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 1\n",
    "%aimport ecg_get_data\n",
    "%aimport Models\n",
    "%aimport train_test_validat1\n",
    "%aimport self_attention\n",
    "%aimport ECGplot\n",
    "\n",
    "import Models \n",
    "from train_test_validat import *\n",
    "from self_attention import *\n",
    "import  ecg_get_data \n",
    "import matplotlib.pyplot as plt\n",
    "import ecg_plot\n",
    "\n",
    "import torch\n",
    "import torch.utils.data as Data\n",
    "import random\n",
    "\n",
    "import time\n",
    "import os\n",
    "import gc\n",
    "\n",
    "random_seed = 2\n",
    "torch.manual_seed(random_seed)    # reproducible\n",
    "torch.cuda.manual_seed_all(random_seed)\n",
    "random.seed(random_seed)\n",
    "np.random.seed(random_seed)\n",
    "\n",
    "time_str = time.strftime(\"%Y%m%d_%H%M%S\", time.localtime()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_npy_path =  './data/train/' #路径\n",
    "test_npy_path =  './data/test/' \n",
    "shadow_npy_folder = './data/shadow/'\n",
    "xml_path = './xml/xml/'\n",
    "#lable_path = './label.npy'\n",
    "\n",
    "model_path = './model/'+time_str\n",
    "log_path = './log/'+  time_str\n",
    "\n",
    "\n",
    "EcgChannles_num = 12\n",
    "EcgLength_num = 5000\n",
    "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "print(torch.cuda.is_available() )# cuda是否可用\n",
    "print(torch.cuda.device_count() )# gpu数量\n",
    "print(torch.cuda.current_device())# 当前设备索引, 从0开始\n",
    "print(torch.cuda.get_device_name(0))# 返回gpu名字\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_Dataset = ecg_get_data.ECG_Dataset(test_npy_path,EcgChannles_num,EcgLength_num,xml_folder=xml_path)\n",
    "x_Dataset = ecg_get_data.ECG_Dataset(train_npy_path,EcgChannles_num,EcgLength_num,shadow_npy_folder=shadow_npy_folder,xml_folder=xml_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "length=len(x_Dataset)\n",
    "train_size,validate_size=int(0.8*length),length-int(0.8*length)\n",
    "#first param is data set to be saperated, the second is list stating how many sets we want it to be.\n",
    "train_Dataset,valid_Dataset=torch.utils.data.random_split(x_Dataset,[train_size,validate_size])\n",
    "print(train_Dataset,valid_Dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ECG,label = test_Dataset.__getitem__(4)\n",
    "#inf,path = train_Dataset.get_basic_inf(55)\n",
    "ecg_plot.plot(ECG*3.5, sample_rate = 500, title = \"test\",row_height= 10,show_grid=True,show_separate_line=True)\n",
    "label\n",
    "#ecg_plot.save_as_png(inf[1],'/workspace/data/OneDrive - mail.hfut.edu.cn/ECG/Interpretable_HTN//PNG_ECG/',dpi = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 16\n",
    "FOLDS = 1\n",
    "EPOCHS = 5000  \n",
    "PATIENCE = 100\n",
    "LR = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter   \n",
    "os.makedirs(model_path, exist_ok=True)\n",
    "writer = SummaryWriter(log_path)\n",
    "from torchsummary import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()# 清空显卡cuda\n",
    "#NET = [Models.resnet18(input_channels=12, inplanes=32, num_classes=2) for i in range(FOLDS)]\n",
    "NET = [Models.channels_split_ATT_CNN() for i in range(FOLDS)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "\n",
    "for fold in range(FOLDS):\n",
    "    \n",
    "    early_stopping = EarlyStopping(PATIENCE, verbose=True, model_path=model_path, delta=0)\n",
    "    #train_dataset,valid_dataset = get_k_fold_dataset(fold=int(fold+1),x = train_x,y=train_y,k=FOLDS,random_seed = random_seed)\n",
    "    #valid_dataset  = ecg_get_data.load_numpy_dataset_to_tensor_dataset(test_x,test_y)\n",
    "    #train_dataset =  ecg_get_data.load_numpy_dataset_to_tensor_dataset(train_x,train_y)\n",
    "    train_dataloader = Data.DataLoader(dataset=train_Dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    valid_dataloader = Data.DataLoader(dataset=test_Dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "    NET[fold].to(DEVICE)\n",
    "    optimizer  = torch.optim.Adadelta(NET[fold].parameters(), lr=LR,weight_decay=1e-2)  \n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 15, gamma=0.1)\n",
    "    criterion = torch.nn.CrossEntropyLoss()   \n",
    "    #等间隔调整学习率\n",
    "    #scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer,T_max = 24)\n",
    "    best_loss = 3\n",
    "    for epoch in range(1,EPOCHS):\n",
    "        time_all=0\n",
    "        start_time = time.time()\n",
    "        train_loss,train_acc = train_model(train_dataloader, NET[fold], criterion, optimizer,DEVICE) # 训练模型\n",
    "        #scheduler.step() # 学习率迭代\n",
    "\n",
    "        time_all = time.time()-start_time\n",
    "        validate_loss,validate_acc = test_model(valid_dataloader,criterion,NET[fold],DEVICE) # 测试模型\n",
    "        writer.add_scalars(main_tag=str(fold)+'_Loss',tag_scalar_dict={'train': train_loss,'validate': validate_loss},global_step=epoch)\n",
    "        writer.add_scalars(main_tag=str(fold)+'_Accuracy',tag_scalar_dict={'train': train_acc,'validate': validate_acc},global_step=epoch)\n",
    "        writer.add_scalars(main_tag=str(fold)+'_LearningRate',tag_scalar_dict={'LR': optimizer.state_dict()['param_groups'][0]['lr']},global_step=epoch)\n",
    "\n",
    "        print('- Epoch: %d - Train_loss: %.5f - Train_acc: %.5f - Val_loss: %.5f - Val_acc: %5f - T_Time: %.3f' %(epoch,train_loss,train_acc,validate_loss,validate_acc,time_all))\n",
    "        print('当前学习率：%f' %optimizer.state_dict()['param_groups'][0]['lr'])\n",
    "\n",
    "        if validate_loss < best_loss:\n",
    "            best_loss = validate_loss\n",
    "            print('Find better model in Epoch {0}, saving model.'.format(epoch))\n",
    "            #torch.save(NET[fold],  model_path+'/best_model_' + str(fold) + '.pt')  # 保存最优模型\n",
    "        else:\n",
    "            scheduler.step() # 学习率迭代\n",
    "        #是否满足早停法条件\n",
    "        if(early_stopping(validate_loss,NET[fold])):\n",
    "            print(\"Early stopping\")\n",
    "            break\n",
    "\n",
    "    print('Fold %d Training Finished' %(fold+1))\n",
    "    torch.cuda.empty_cache()# 清空显卡cuda\n",
    "print('Training Finished')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_Dataset.__len__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_Dataset.__len__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_Dataset.__len__()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.13 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
