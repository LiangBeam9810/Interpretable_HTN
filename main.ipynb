{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 1\n",
    "%aimport ecg_get_data\n",
    "%aimport Models\n",
    "%aimport train_test_validat\n",
    "%aimport self_attention\n",
    "%aimport ECGplot\n",
    "%aimport Net\n",
    "%aimport select_dataset\n",
    "import select_dataset\n",
    "import Models \n",
    "import Net\n",
    "from train_test_validat import *\n",
    "from self_attention import *\n",
    "import  ecg_get_data \n",
    "import matplotlib.pyplot as plt\n",
    "import ecg_plot\n",
    "\n",
    "import torch\n",
    "import torch.utils.data as Data\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "import random\n",
    "\n",
    "import time\n",
    "import os\n",
    "import gc\n",
    "\n",
    "# random_seed = 2\n",
    "# torch.manual_seed(random_seed)    # reproducible\n",
    "# torch.cuda.manual_seed_all(random_seed)\n",
    "# random.seed(random_seed)\n",
    "# np.random.seed(random_seed)\n",
    "\n",
    "time_str = time.strftime(\"%Y%m%d_%H%M%S\", time.localtime()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = './model/'+time_str\n",
    "log_path = './log/'+  time_str\n",
    "ECG_root = '/workspace/data/Preprocess_HTN/dataII/ECG'\n",
    "EcgChannles_num = 12\n",
    "EcgLength_num = 5000\n",
    "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = select_dataset.splite_dataset('/workspace/data/Preprocess_HTN/dataII/',True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_list = data.__get_test_file_list__(True,'外科')\n",
    "valid_list,train_list,addition_train_list = data.__get_VT_file_list__(0.9,True,'外科')  # type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_Dataset = ecg_get_data.ECG_Dataset(ECG_root,valid_list,EcgChannles_num,EcgLength_num,position_encode=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_Dataset = ecg_get_data.ECG_Dataset(ECG_root,test_list,EcgChannles_num,EcgLength_num,position_encode=False)\n",
    "# valid_Dataset = ecg_get_data.ECG_Dataset(ECG_root,valid_list,EcgChannles_num,EcgLength_num,position_encode=False)\n",
    "train_Dataset = ecg_get_data.ECG_Dataset(ECG_root,train_list,EcgChannles_num,EcgLength_num,shadow_npy_folder=ECG_root,shadow_npy_files_list=addition_train_list,position_encode=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ECG,label = train_Dataset.__getitem__(17)\n",
    "ecg_plot.plot(ECG*3500/1000, sample_rate = 500, title = \"test\",row_height= 10,show_grid=True,show_separate_line=True)\n",
    "#label\n",
    "#ecg_plot.save_as_png(inf[1],'/workspace/data/OneDrive - mail.hfut.edu.cn/ECG/Interpretable_HTN//PNG_ECG/',dpi = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_Dataset.ECG.shape\n",
    "train_Dataset.ECG.shape\n",
    "test_Dataset.ECG.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128\n",
    "FOLDS = 1\n",
    "EPOCHS = 5000  \n",
    "PATIENCE = 500\n",
    "LR = 0.01\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()# 清空显卡cuda\n",
    "# NET = [\n",
    "#     Net.channels_branch_CNN(True,res = True,se = True,Dropout_rate = 0.25)\n",
    "# ]\n",
    "NET = [Net.MLBFNet(True,res = True,se = True,Dropout_rate = 0.25) ] # type: ignore\n",
    "# NET = [Net.MLBFNet(True,res = True,se = True,Dropout_rate = 0.25) ] # type: ignore\n",
    "# NET = [Models.ECGNet(mark=True ,res = True,se = True) ] # type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "os.makedirs(model_path, exist_ok=True)\n",
    "writer = SummaryWriter(log_path)\n",
    "# writer.add_graph(NET[0], torch.zeros((1,12,5000)))  #模型及模型输入数据\n",
    "torch.cuda.empty_cache()# 清空显卡cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "def linear_combination(x, y, epsilon): \n",
    "    return epsilon*x + (1-epsilon)*y\n",
    "def reduce_loss(loss, reduction='mean'):\n",
    "    return loss.mean() if reduction=='mean' else loss.sum() if reduction=='sum' else loss\n",
    "\n",
    "\n",
    "class LabelSmoothingCrossEntropy(nn.Module):\n",
    "    def __init__(self, epsilon:float=0.1, reduction='mean'):\n",
    "        super().__init__()\n",
    "        self.epsilon = epsilon\n",
    "        self.reduction = reduction\n",
    "    \n",
    "    def forward(self, preds, target):\n",
    "        n = preds.size()[-1]\n",
    "        log_preds = F.log_softmax(preds, dim=-1)\n",
    "        loss = reduce_loss(-log_preds.sum(dim=-1), self.reduction)\n",
    "        nll = F.nll_loss(log_preds, target, reduction=self.reduction)\n",
    "        return linear_combination(loss/n, nll, self.epsilon)\n",
    "    \n",
    "class GCELoss(nn.Module):\n",
    "    def __init__(self, num_classes=2, q=0.7):\n",
    "        super(GCELoss, self).__init__()\n",
    "        self.q = q\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "    def forward(self, pred, labels):\n",
    "        pred = torch.nn.functional.softmax(pred, dim=1)\n",
    "        pred = torch.clamp(pred, min=1e-4, max=1.0)\n",
    "        label_one_hot = torch.nn.functional.one_hot(labels, self.num_classes).float().to(pred.device)\n",
    "        loss = (1. - torch.pow(torch.sum(label_one_hot * pred, dim=1), self.q)) / self.q\n",
    "        return loss.mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "for fold in range(FOLDS):\n",
    "    #每个人fold都重新抽取\n",
    "    NET[fold].to(DEVICE)\n",
    "    test_list = data.__get_test_file_list__(True,'外科')\n",
    "    valid_list,train_list,addition_train_list = data.__get_VT_file_list__(0.9,True,'外科')  # type: ignore\n",
    "    \n",
    "    test_Dataset = ecg_get_data.ECG_Dataset(ECG_root,test_list,EcgChannles_num,EcgLength_num)\n",
    "    valid_Dataset = ecg_get_data.ECG_Dataset(ECG_root,valid_list,EcgChannles_num,EcgLength_num)\n",
    "    valid_dataloader = Data.DataLoader(dataset=valid_Dataset, batch_size=BATCH_SIZE, shuffle=True,num_workers=4,pin_memory=True)\n",
    "    test_dataloader = Data.DataLoader(dataset=test_Dataset, batch_size=BATCH_SIZE, shuffle=True,num_workers=4,pin_memory=True)\n",
    "    \n",
    "    # train_Dataset = ecg_get_data.ECG_Dataset(ECG_root,train_list,EcgChannles_num,EcgLength_num,shadow_npy_folder=ECG_root,shadow_npy_files_list=addition_train_list)\n",
    "    # train_dataloader = Data.DataLoader(dataset=train_Dataset, batch_size=BATCH_SIZE, shuffle=True,num_workers=4,pin_memory=True)\n",
    "    \n",
    "    early_stopping = EarlyStopping(PATIENCE, verbose=True, model_path=model_path, delta=0, positive=False)\n",
    "    optimizer  = torch.optim.AdamW(NET[fold].parameters(), lr=LR,weight_decay=1e-2)  \n",
    "    \n",
    "    warm_up_iter = 10\n",
    "    T_max = 500\t# 周期\n",
    "    lr_max = 1e-3\t# 最大值\n",
    "    lr_min = 1e-5\t# 最小值\n",
    "    lambda0 = lambda cur_iter: lr_min if  cur_iter < warm_up_iter else \\\n",
    "        (lr_min + 0.5*(lr_max-lr_min)*(1.0+math.cos( (cur_iter-warm_up_iter)/(T_max-warm_up_iter)*math.pi)))/0.01\n",
    "    scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=lambda0)\n",
    "    criterion = torch.nn.CrossEntropyLoss()   \n",
    "    # criterion = LabelSmoothingCrossEntropy()\n",
    "    train_HTN_list = list(filter(lambda x: x.find('_HTN.') >= 0, train_list))  # type: ignore\n",
    "    candidate_NHTN_list = list(filter(lambda x: x.find('_NHTN.') >= 0, train_list)) + addition_train_list # type: ignore\n",
    "    best_test_F1 = 0\n",
    "    for epoch in range(1,EPOCHS):\n",
    "        #每个epoch 更新train中的NHTN类 大概13s\n",
    "        time_all=0\n",
    "        train_Dataset = ecg_get_data.ECG_Dataset(ECG_root,train_list,EcgChannles_num,EcgLength_num,shadow_npy_folder=ECG_root,shadow_npy_files_list=addition_train_list)\n",
    "        train_dataloader = Data.DataLoader(dataset=train_Dataset, batch_size=BATCH_SIZE, shuffle=True,num_workers=4,pin_memory=True) \n",
    "        \n",
    "        start_time = time.time()\n",
    "\n",
    "        #y_true,y_pred,train_loss,train_acc = train_model(train_dataloader, NET[fold], criterion, optimizer,DEVICE) # type: ignore # 训练模型\n",
    "        y_true,y_pred,train_loss,train_acc = train_model(train_dataloader, NET[fold], criterion, optimizer,DEVICE) # type: ignore # 训练模型\n",
    "        time_all = time.time()-start_time\n",
    "        F1_score_train =f1_score(y_true, y_pred, average='macro')#F1分数\n",
    "        C0 = confusion_matrix(y_true,y_pred)\n",
    "        y_true,y_pred,validate_loss,validate_acc = eval_model(valid_dataloader,criterion,NET[fold],DEVICE) # 验证模型\n",
    "        F1_score_valid =f1_score(y_true, y_pred, average='macro')#F1分数\n",
    "        C1 = confusion_matrix(y_true,y_pred)\n",
    "        y_true,y_pred,test_loss,test_acc = eval_model(test_dataloader,criterion,NET[fold],DEVICE) # 验证模型\n",
    "        F1_score_test =f1_score(y_true, y_pred, average='macro')#F1分数\n",
    "        C2 = confusion_matrix(y_true,y_pred)\n",
    "\n",
    "        # writer.add_scalars(main_tag=str(fold)+'_Loss',tag_scalar_dict={'train': train_loss,'validate': validate_loss},global_step=epoch)\n",
    "        # writer.add_scalars(main_tag=str(fold)+'_Accuracy',tag_scalar_dict={'train': train_acc,'validate': validate_acc},global_step=epoch)\n",
    "        # writer.add_scalars(main_tag=str(fold)+'_LearningRate',tag_scalar_dict={'LR': optimizer.state_dict()['param_groups'][0]['lr']},global_step=epoch)\n",
    "        # writer.add_scalars(main_tag=str(fold)+'_F1_score',tag_scalar_dict={'train':F1_score_train,'validate': F1_score_valid},global_step=epoch)\n",
    "        writer.add_scalars(main_tag=str(fold)+'_Loss',tag_scalar_dict={'train': train_loss,'validate': validate_loss,'test':test_loss},global_step=epoch)\n",
    "        writer.add_scalars(main_tag=str(fold)+'_Accuracy',tag_scalar_dict={'train': train_acc,'validate': validate_acc,'test':test_acc},global_step=epoch)\n",
    "        writer.add_scalars(main_tag=str(fold)+'_LearningRate',tag_scalar_dict={'LR': optimizer.state_dict()['param_groups'][0]['lr']},global_step=epoch)\n",
    "        writer.add_scalars(main_tag=str(fold)+'_F1_score',tag_scalar_dict={'train':F1_score_train,'validate': F1_score_valid,'test':F1_score_test},global_step=epoch)        \n",
    "        print('- Epoch: %d - Train_loss: %.5f - Train_acc: %.5f - F1 score: %.5f - Val_loss: %.5f - Val_acc: %.5f - F1 score: %.5f - T_Time: %.5f' %(epoch,train_loss,train_acc,F1_score_train,validate_loss,validate_acc,F1_score_valid,time_all))\n",
    "        print('当前学习率：%.8f' %optimizer.state_dict()['param_groups'][0]['lr'])\n",
    "        print('train:\\n',C0)\n",
    "        print('validate:\\n',C1)\n",
    "        print('test:\\n',C2)\n",
    "        \n",
    "        if(F1_score_test>best_test_F1):\n",
    "            best_test_F1 = F1_score_test\n",
    "            torch.save(NET[fold].state_dict(), model_path+'/parameter_best_test_' + str(fold) + '.pt')\n",
    "        \n",
    "        scheduler.step() # 学习率迭代\n",
    "        #是否满足早停法条件\n",
    "        if(early_stopping(validate_loss,NET[fold],fold)):\n",
    "            print(\"Early stopping\")\n",
    "            break\n",
    "        if(epoch>T_max):\n",
    "            break\n",
    "        train_list = data.pair_HTN_by_list_(train_HTN_list,candidate_NHTN_list,5)\n",
    "    print('Fold %d Training Finished' %(fold+1))\n",
    "    torch.cuda.empty_cache()# 清空显卡cuda\n",
    "print('Training Finished')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
