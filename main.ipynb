{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 1\n",
    "%aimport ecg_get_data\n",
    "%aimport Models\n",
    "%aimport train_test_validat\n",
    "%aimport self_attention\n",
    "%aimport ECGplot\n",
    "import Models \n",
    "from train_test_validat import *\n",
    "from self_attention import *\n",
    "from  ecg_get_data import *\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.utils.data as Data\n",
    "import random\n",
    "\n",
    "import time\n",
    "random_seed = 1\n",
    "torch.manual_seed(random_seed)    # reproducible\n",
    "torch.cuda.manual_seed_all(random_seed)\n",
    "random.seed(random_seed)\n",
    "np.random.seed(random_seed)\n",
    "\n",
    "time_str = time.strftime(\"%Y%m%d_%H%M%S\", time.localtime()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data_path =  './npy_ECG/' #路径\n",
    "lable_path = './label.npy'\n",
    "model_path = './model/'+time_str\n",
    "log_path = './log/'+  time_str\n",
    "\n",
    "\n",
    "EcgChannles_num = 12\n",
    "EcgLength_num = 5000\n",
    "DEVICE = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = load_data(data_path,EcgChannles_num=EcgChannles_num,EcgLength_num=EcgLength_num)\n",
    "y = load_label(lable_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train_x,test_x,train_y,test_y = train_test_split(x,y,train_size=0.9,random_state = random_seed,shuffle = True,stratify=y)\n",
    "print(\"         HTN     NHTN \")\n",
    "print(\"train: %5d   %5d\" % (train_y.sum(),len(train_y)-train_y.sum()))\n",
    "print(\"test : %5d   %5d\" % (test_y.sum(),len(test_y)-test_y.sum()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_dataset,valid_dataset = load_numpy_dataset_to_tensor_dataset(x,y,random_seed=random_seed,train_rate = 0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128\n",
    "FOLDS = 1\n",
    "EPOCHS = 2000  \n",
    "PATIENCE = 64\n",
    "LR = 0.00001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter   \n",
    "os.makedirs(model_path, exist_ok=True)\n",
    "writer = SummaryWriter(log_path)\n",
    "from torchsummary import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NET = [Models.CNN_ATT7() for i in range(FOLDS)]\n",
    "print(summary(NET[0], (EcgChannles_num,EcgLength_num), device=\"cpu\"))\n",
    "writer.add_graph(NET[0],torch.rand([1,EcgChannles_num,EcgLength_num]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "\n",
    "for fold in range(FOLDS):\n",
    "    \n",
    "    early_stopping = EarlyStopping(PATIENCE, verbose=True, model_path=model_path, delta=0)\n",
    "    train_dataset,valid_dataset = get_k_fold_dataset(fold=int(fold+1),x = train_x,y=train_y,k=FOLDS,random_seed = random_seed)\n",
    "    train_dataloader = Data.DataLoader(dataset=train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    valid_dataloader = Data.DataLoader(dataset=valid_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "    NET[fold].to(DEVICE)\n",
    "    optimizer  = torch.optim.Adam(NET[fold].parameters(), lr=LR)  \n",
    "    criterion = torch.nn.CrossEntropyLoss()   \n",
    "    #等间隔调整学习率\n",
    "    #scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer,T_max = 32)\n",
    "    best_loss = 3\n",
    "    for epoch in range(1,EPOCHS):\n",
    "        time_all=0\n",
    "        start_time = time.time()\n",
    "        train_loss,train_acc = train_model(train_dataloader, NET[fold], criterion, optimizer,DEVICE) # 训练模型\n",
    "        #scheduler.step() # 学习率迭代\n",
    "\n",
    "        time_all = time.time()-start_time\n",
    "        validate_loss,validate_acc = test_model(train_dataloader,criterion,NET[fold],DEVICE) # 测试模型\n",
    "        writer.add_scalars(main_tag=str(fold)+'_Loss',tag_scalar_dict={'train': train_loss,'validate': validate_loss},global_step=epoch)\n",
    "        writer.add_scalars(main_tag=str(fold)+'_Accuracy',tag_scalar_dict={'train': train_acc,'validate': validate_acc},global_step=epoch)\n",
    "        writer.add_scalars(main_tag=str(fold)+'_LearningRate',tag_scalar_dict={'LR': optimizer.state_dict()['param_groups'][0]['lr']},global_step=epoch)\n",
    "\n",
    "        print('- Epoch: %d - Train_loss: %.5f - Train_acc: %.5f - Val_loss: %.5f - Val_acc: %5f - T_Time: %.3f' %(epoch,train_loss,train_acc,validate_loss,validate_acc,time_all))\n",
    "        print('当前学习率：%f' %optimizer.state_dict()['param_groups'][0]['lr'])\n",
    "\n",
    "        if validate_loss < best_loss:\n",
    "            best_loss = validate_loss\n",
    "            print('Find better model in Epoch {0}, saving model.'.format(epoch))\n",
    "            torch.save(NET[fold],  model_path+'/best_model_' + str(fold) + '.pt')  # 保存最优模型\n",
    "        #是否满足早停法条件\n",
    "        if(early_stopping(validate_loss,NET[fold])):\n",
    "            print(\"Early stopping\")\n",
    "            break\n",
    "\n",
    "    print('Fold %d Training Finished' %(fold+1))\n",
    "    torch.cuda.empty_cache()# 清空显卡cuda\n",
    "print('Training Finished')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ------Display the attention value-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# -----For Leads -----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_model_path = \"./model/20220907_091538/best_model_0.pt\"\n",
    "test_model = torch.load(test_model_path).to(DEVICE)\n",
    "test_x = x[0:1698]\n",
    "test_y = x[0:1698]\n",
    "test_x = MAX_MIN_normalization_by_feactures(test_x)\n",
    "test_x = torch.FloatTensor(test_x)  #turn numpy to tensor\n",
    "test_y = torch.LongTensor(test_y)\n",
    "test_dataset = Data.TensorDataset(test_x, test_y)\n",
    "test_dataloader = Data.DataLoader(dataset=test_dataset, batch_size=1, shuffle=False)\n",
    "test_model.eval()\n",
    "\n",
    "test_model.eval()\n",
    "attention_value_timestep = np.zeros(EcgChannles_num,)\n",
    "for i,data in enumerate(test_dataloader,0):\n",
    "    inputs,labels = data[0].to(DEVICE),data[1].to(DEVICE)\n",
    "    outputs = test_model(inputs)\n",
    "    #print(outputs)\n",
    "    _,pred = outputs.max(1) # 求概率最大值对应的标签\n",
    "    #print(\"the label :{labels},pred is {pred}\".format(labels=labels[0],pred=pred[0]))\n",
    "    attention_value = test_model.attention_value\n",
    "    attention_value_timestep += (((attention_value.to('cpu'))[0]).detach().numpy()).sum(axis=0)\n",
    "plt.figure(figsize=(5,5))\n",
    "plt.stem(np.arange(0,len(attention_value_timestep)), attention_value_timestep)\n",
    "plt.xticks(np.arange(0,len(attention_value_timestep)),['I', 'II', 'III', 'aVR', 'aVL', 'aVF', 'V1', 'V2', 'V3', 'V4', 'V5', 'V6'])\n",
    "plt.ylabel(\"Sum of attention value\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(summary(Models.CNN_ATT7(), (EcgChannles_num,EcgLength_num), device=\"cpu\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.13 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
