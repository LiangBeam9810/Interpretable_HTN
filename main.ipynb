{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 1\n",
    "%aimport ecg_get_data\n",
    "%aimport Models\n",
    "%aimport train_test_validat\n",
    "%aimport self_attention\n",
    "%aimport ECGplot\n",
    "import Models \n",
    "from train_test_validat import *\n",
    "from self_attention import *\n",
    "from  ecg_get_data import *\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.utils.data as Data\n",
    "import random\n",
    "\n",
    "import time\n",
    "random_seed = 2\n",
    "torch.manual_seed(random_seed)    # reproducible\n",
    "torch.cuda.manual_seed_all(random_seed)\n",
    "random.seed(random_seed)\n",
    "np.random.seed(random_seed)\n",
    "\n",
    "time_str = time.strftime(\"%Y%m%d_%H%M%S\", time.localtime()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:1\n"
     ]
    }
   ],
   "source": [
    "\n",
    "data_path =  './npy_ECG/' #路径\n",
    "lable_path = './label.npy'\n",
    "model_path = './model/'+time_str\n",
    "log_path = './log/'+  time_str\n",
    "\n",
    "\n",
    "EcgChannles_num = 12\n",
    "EcgLength_num = 5000\n",
    "DEVICE = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1698/1698 [00:16<00:00, 104.41it/s]\n"
     ]
    }
   ],
   "source": [
    "x = load_data(data_path,EcgChannles_num=EcgChannles_num,EcgLength_num=EcgLength_num)\n",
    "y = load_label(lable_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         HTN     NHTN \n",
      "train:   628     900\n",
      "test :    70     100\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train_x,test_x,train_y,test_y = train_test_split(x,y,train_size=0.9,random_state = random_seed,shuffle = True,stratify=y)\n",
    "print(\"         HTN     NHTN \")\n",
    "print(\"train: %5d   %5d\" % (train_y.sum(),len(train_y)-train_y.sum()))\n",
    "print(\"test : %5d   %5d\" % (test_y.sum(),len(test_y)-test_y.sum()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_dataset,valid_dataset = load_numpy_dataset_to_tensor_dataset(x,y,random_seed=random_seed,train_rate = 0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128\n",
    "FOLDS = 1\n",
    "EPOCHS = 2000  \n",
    "PATIENCE = 50\n",
    "LR = 0.00001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter   \n",
    "os.makedirs(model_path, exist_ok=True)\n",
    "writer = SummaryWriter(log_path)\n",
    "from torchsummary import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "       BatchNorm1d-1             [-1, 12, 5000]              24\n",
      "            Conv1d-2             [-1, 32, 5000]           1,952\n",
      "              ReLU-3             [-1, 32, 5000]               0\n",
      "         MaxPool1d-4             [-1, 32, 1250]               0\n",
      "       BatchNorm1d-5             [-1, 32, 1250]              64\n",
      "            Conv1d-6             [-1, 64, 1250]           6,208\n",
      "              ReLU-7             [-1, 64, 1250]               0\n",
      "         MaxPool1d-8              [-1, 64, 312]               0\n",
      "       BatchNorm1d-9              [-1, 64, 312]             128\n",
      "           Conv1d-10              [-1, 64, 312]           4,160\n",
      "           Conv1d-11              [-1, 64, 312]           4,160\n",
      "           Conv1d-12              [-1, 64, 312]           4,160\n",
      "          Softmax-13             [-1, 312, 312]               0\n",
      "      BatchNorm1d-14              [-1, 64, 312]             128\n",
      "self_Attention_1D_for_timestep_without_relu-15  [[-1, 64, 312], [-1, 312, 312]]               0\n",
      "          Dropout-16              [-1, 64, 312]               0\n",
      "           Linear-17                 [-1, 1024]      20,448,256\n",
      "             ReLU-18                 [-1, 1024]               0\n",
      "           Linear-19                  [-1, 128]         131,200\n",
      "             ReLU-20                  [-1, 128]               0\n",
      "           Linear-21                    [-1, 2]             258\n",
      "          Softmax-22                    [-1, 2]               0\n",
      "================================================================\n",
      "Total params: 20,600,698\n",
      "Trainable params: 20,600,698\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.23\n",
      "Forward/backward pass size (MB): 14823.19\n",
      "Params size (MB): 78.59\n",
      "Estimated Total Size (MB): 14902.01\n",
      "----------------------------------------------------------------\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "NET = [Models.CNN_ATT7() for i in range(FOLDS)]\n",
    "print(summary(NET[0], (EcgChannles_num,EcgLength_num), device=\"cpu\"))\n",
    "writer.add_graph(NET[0],torch.rand([1,EcgChannles_num,EcgLength_num]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1528\n",
      "               HTN  NHTN\n",
      "valid_dataset: 125  181\n",
      "train_dataset: 503  1097\n",
      "- Epoch: 1 - Train_loss: 0.66878 - Train_acc: 0.61609 - Val_loss: 0.68235 - Val_acc: 0.687604 - T_Time: 2.304\n",
      "当前学习率：0.000010\n",
      "Find better model in Epoch 1, saving model.\n",
      "Validation loss decreased (inf --> 0.682354).  Saving model ...\n",
      "                    --------------------------------------------------\n",
      "\n",
      "- Epoch: 2 - Train_loss: 0.63919 - Train_acc: 0.64567 - Val_loss: 0.65416 - Val_acc: 0.658646 - T_Time: 0.645\n",
      "当前学习率：0.000010\n",
      "Find better model in Epoch 2, saving model.\n",
      "Validation loss decreased (0.682354 --> 0.654157).  Saving model ...\n",
      "                    --------------------------------------------------\n",
      "\n",
      "- Epoch: 3 - Train_loss: 0.62516 - Train_acc: 0.66402 - Val_loss: 0.63098 - Val_acc: 0.666771 - T_Time: 0.644\n",
      "当前学习率：0.000010\n",
      "Find better model in Epoch 3, saving model.\n",
      "Validation loss decreased (0.654157 --> 0.630977).  Saving model ...\n",
      "                    --------------------------------------------------\n",
      "\n",
      "- Epoch: 4 - Train_loss: 0.61635 - Train_acc: 0.68096 - Val_loss: 0.63361 - Val_acc: 0.642396 - T_Time: 0.634\n",
      "当前学习率：0.000010\n",
      "EarlyStopping counter: 1 out of 50\n",
      "\n",
      "- Epoch: 5 - Train_loss: 0.60827 - Train_acc: 0.68991 - Val_loss: 0.61466 - Val_acc: 0.676042 - T_Time: 0.625\n",
      "当前学习率：0.000010\n",
      "Find better model in Epoch 5, saving model.\n",
      "Validation loss decreased (0.630977 --> 0.614658).  Saving model ...\n",
      "                    --------------------------------------------------\n",
      "\n",
      "- Epoch: 6 - Train_loss: 0.59798 - Train_acc: 0.71491 - Val_loss: 0.61933 - Val_acc: 0.665313 - T_Time: 0.602\n",
      "当前学习率：0.000010\n",
      "EarlyStopping counter: 1 out of 50\n",
      "\n",
      "- Epoch: 7 - Train_loss: 0.58984 - Train_acc: 0.72978 - Val_loss: 0.62667 - Val_acc: 0.659792 - T_Time: 0.631\n",
      "当前学习率：0.000010\n",
      "EarlyStopping counter: 2 out of 50\n",
      "\n",
      "- Epoch: 8 - Train_loss: 0.57951 - Train_acc: 0.75268 - Val_loss: 0.62148 - Val_acc: 0.673125 - T_Time: 0.627\n",
      "当前学习率：0.000010\n",
      "EarlyStopping counter: 3 out of 50\n",
      "\n",
      "- Epoch: 9 - Train_loss: 0.56792 - Train_acc: 0.77623 - Val_loss: 0.61388 - Val_acc: 0.669375 - T_Time: 0.623\n",
      "当前学习率：0.000010\n",
      "Find better model in Epoch 9, saving model.\n",
      "Validation loss decreased (0.614658 --> 0.613883).  Saving model ...\n",
      "                    --------------------------------------------------\n",
      "\n",
      "- Epoch: 10 - Train_loss: 0.56019 - Train_acc: 0.79835 - Val_loss: 0.61590 - Val_acc: 0.682396 - T_Time: 0.609\n",
      "当前学习率：0.000010\n",
      "EarlyStopping counter: 1 out of 50\n",
      "\n",
      "- Epoch: 11 - Train_loss: 0.55119 - Train_acc: 0.80879 - Val_loss: 0.61455 - Val_acc: 0.685000 - T_Time: 0.604\n",
      "当前学习率：0.000010\n",
      "EarlyStopping counter: 2 out of 50\n",
      "\n",
      "- Epoch: 12 - Train_loss: 0.54123 - Train_acc: 0.82375 - Val_loss: 0.61317 - Val_acc: 0.694271 - T_Time: 0.649\n",
      "当前学习率：0.000010\n",
      "Find better model in Epoch 12, saving model.\n",
      "Validation loss decreased (0.613883 --> 0.613174).  Saving model ...\n",
      "                    --------------------------------------------------\n",
      "\n",
      "- Epoch: 13 - Train_loss: 0.53054 - Train_acc: 0.85484 - Val_loss: 0.60275 - Val_acc: 0.690521 - T_Time: 0.645\n",
      "当前学习率：0.000010\n",
      "Find better model in Epoch 13, saving model.\n",
      "Validation loss decreased (0.613174 --> 0.602752).  Saving model ...\n",
      "                    --------------------------------------------------\n",
      "\n",
      "- Epoch: 14 - Train_loss: 0.52077 - Train_acc: 0.86031 - Val_loss: 0.60708 - Val_acc: 0.679792 - T_Time: 0.657\n",
      "当前学习率：0.000010\n",
      "EarlyStopping counter: 1 out of 50\n",
      "\n",
      "- Epoch: 15 - Train_loss: 0.51342 - Train_acc: 0.86710 - Val_loss: 0.61102 - Val_acc: 0.662396 - T_Time: 0.647\n",
      "当前学习率：0.000010\n",
      "EarlyStopping counter: 2 out of 50\n",
      "\n",
      "- Epoch: 16 - Train_loss: 0.50373 - Train_acc: 0.87987 - Val_loss: 0.60987 - Val_acc: 0.681250 - T_Time: 0.612\n",
      "当前学习率：0.000010\n",
      "EarlyStopping counter: 3 out of 50\n",
      "\n",
      "- Epoch: 17 - Train_loss: 0.49152 - Train_acc: 0.90263 - Val_loss: 0.60896 - Val_acc: 0.677188 - T_Time: 0.648\n",
      "当前学习率：0.000010\n",
      "EarlyStopping counter: 4 out of 50\n",
      "\n",
      "- Epoch: 18 - Train_loss: 0.48276 - Train_acc: 0.90848 - Val_loss: 0.61592 - Val_acc: 0.667604 - T_Time: 0.648\n",
      "当前学习率：0.000010\n",
      "EarlyStopping counter: 5 out of 50\n",
      "\n",
      "- Epoch: 19 - Train_loss: 0.47642 - Train_acc: 0.91734 - Val_loss: 0.61415 - Val_acc: 0.670521 - T_Time: 0.609\n",
      "当前学习率：0.000010\n",
      "EarlyStopping counter: 6 out of 50\n",
      "\n",
      "- Epoch: 20 - Train_loss: 0.46758 - Train_acc: 0.91616 - Val_loss: 0.60356 - Val_acc: 0.678646 - T_Time: 0.615\n",
      "当前学习率：0.000010\n",
      "EarlyStopping counter: 7 out of 50\n",
      "\n",
      "- Epoch: 21 - Train_loss: 0.45802 - Train_acc: 0.93243 - Val_loss: 0.60042 - Val_acc: 0.697188 - T_Time: 0.634\n",
      "当前学习率：0.000010\n",
      "Find better model in Epoch 21, saving model.\n",
      "Validation loss decreased (0.602752 --> 0.600416).  Saving model ...\n",
      "                    --------------------------------------------------\n",
      "\n",
      "- Epoch: 22 - Train_loss: 0.45084 - Train_acc: 0.93779 - Val_loss: 0.60567 - Val_acc: 0.699792 - T_Time: 0.659\n",
      "当前学习率：0.000010\n",
      "EarlyStopping counter: 1 out of 50\n",
      "\n",
      "- Epoch: 23 - Train_loss: 0.44143 - Train_acc: 0.95625 - Val_loss: 0.61738 - Val_acc: 0.665000 - T_Time: 0.614\n",
      "当前学习率：0.000010\n",
      "EarlyStopping counter: 2 out of 50\n",
      "\n",
      "- Epoch: 24 - Train_loss: 0.43432 - Train_acc: 0.95913 - Val_loss: 0.61595 - Val_acc: 0.662396 - T_Time: 0.623\n",
      "当前学习率：0.000010\n",
      "EarlyStopping counter: 3 out of 50\n",
      "\n",
      "- Epoch: 25 - Train_loss: 0.42648 - Train_acc: 0.96603 - Val_loss: 0.60590 - Val_acc: 0.680104 - T_Time: 0.645\n",
      "当前学习率：0.000010\n",
      "EarlyStopping counter: 4 out of 50\n",
      "\n",
      "- Epoch: 26 - Train_loss: 0.41886 - Train_acc: 0.97136 - Val_loss: 0.61373 - Val_acc: 0.678646 - T_Time: 0.629\n",
      "当前学习率：0.000010\n",
      "EarlyStopping counter: 5 out of 50\n",
      "\n",
      "- Epoch: 27 - Train_loss: 0.41151 - Train_acc: 0.97578 - Val_loss: 0.60205 - Val_acc: 0.688229 - T_Time: 0.609\n",
      "当前学习率：0.000010\n",
      "EarlyStopping counter: 6 out of 50\n",
      "\n",
      "- Epoch: 28 - Train_loss: 0.40750 - Train_acc: 0.97592 - Val_loss: 0.61427 - Val_acc: 0.687604 - T_Time: 0.623\n",
      "当前学习率：0.000010\n",
      "EarlyStopping counter: 7 out of 50\n",
      "\n",
      "- Epoch: 29 - Train_loss: 0.39913 - Train_acc: 0.98047 - Val_loss: 0.61879 - Val_acc: 0.653125 - T_Time: 0.617\n",
      "当前学习率：0.000010\n",
      "EarlyStopping counter: 8 out of 50\n",
      "\n",
      "- Epoch: 30 - Train_loss: 0.39619 - Train_acc: 0.98203 - Val_loss: 0.62536 - Val_acc: 0.656563 - T_Time: 0.586\n",
      "当前学习率：0.000010\n",
      "EarlyStopping counter: 9 out of 50\n",
      "\n",
      "- Epoch: 31 - Train_loss: 0.39026 - Train_acc: 0.98217 - Val_loss: 0.61399 - Val_acc: 0.671667 - T_Time: 0.637\n",
      "当前学习率：0.000010\n",
      "EarlyStopping counter: 10 out of 50\n",
      "\n",
      "- Epoch: 32 - Train_loss: 0.38588 - Train_acc: 0.98230 - Val_loss: 0.60998 - Val_acc: 0.675729 - T_Time: 0.634\n",
      "当前学习率：0.000010\n",
      "EarlyStopping counter: 11 out of 50\n",
      "\n",
      "- Epoch: 33 - Train_loss: 0.38202 - Train_acc: 0.98478 - Val_loss: 0.62073 - Val_acc: 0.658333 - T_Time: 0.612\n",
      "当前学习率：0.000010\n",
      "EarlyStopping counter: 12 out of 50\n",
      "\n",
      "- Epoch: 34 - Train_loss: 0.37748 - Train_acc: 0.98556 - Val_loss: 0.61202 - Val_acc: 0.686458 - T_Time: 0.613\n",
      "当前学习率：0.000010\n",
      "EarlyStopping counter: 13 out of 50\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_5716/3262644988.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mtime_all\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0mtrain_loss\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNET\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfold\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mDEVICE\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# 训练模型\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m         \u001b[0;31m#scheduler.step() # 学习率迭代\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/workspace/data/Interpretable_HTN/train_test_validat.py\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(train_loader, model, criterion, optimizer, device)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# 梯度清0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# 反向传播\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# 更新系数\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    394\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    395\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 396\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    397\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    398\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    173\u001b[0m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[1;32m    174\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 175\u001b[0;31m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    176\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m def grad(\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "\n",
    "for fold in range(FOLDS):\n",
    "    \n",
    "    early_stopping = EarlyStopping(PATIENCE, verbose=True, model_path=model_path, delta=0)\n",
    "    train_dataset,valid_dataset = get_k_fold_dataset(fold=int(fold+1),x = train_x,y=train_y,k=FOLDS,random_seed = random_seed)\n",
    "    train_dataloader = Data.DataLoader(dataset=train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    valid_dataloader = Data.DataLoader(dataset=valid_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "    NET[fold].to(DEVICE)\n",
    "    optimizer  = torch.optim.Adam(NET[fold].parameters(), lr=LR)  \n",
    "    criterion = torch.nn.CrossEntropyLoss()   \n",
    "    #等间隔调整学习率\n",
    "    #scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer,T_max = 32)\n",
    "    best_loss = 3\n",
    "    for epoch in range(1,EPOCHS):\n",
    "        time_all=0\n",
    "        start_time = time.time()\n",
    "        train_loss,train_acc = train_model(train_dataloader, NET[fold], criterion, optimizer,DEVICE) # 训练模型\n",
    "        #scheduler.step() # 学习率迭代\n",
    "\n",
    "        time_all = time.time()-start_time\n",
    "        validate_loss,validate_acc = test_model(valid_dataloader,criterion,NET[fold],DEVICE) # 测试模型\n",
    "        writer.add_scalars(main_tag=str(fold)+'_Loss',tag_scalar_dict={'train': train_loss,'validate': validate_loss},global_step=epoch)\n",
    "        writer.add_scalars(main_tag=str(fold)+'_Accuracy',tag_scalar_dict={'train': train_acc,'validate': validate_acc},global_step=epoch)\n",
    "        writer.add_scalars(main_tag=str(fold)+'_LearningRate',tag_scalar_dict={'LR': optimizer.state_dict()['param_groups'][0]['lr']},global_step=epoch)\n",
    "\n",
    "        print('- Epoch: %d - Train_loss: %.5f - Train_acc: %.5f - Val_loss: %.5f - Val_acc: %5f - T_Time: %.3f' %(epoch,train_loss,train_acc,validate_loss,validate_acc,time_all))\n",
    "        print('当前学习率：%f' %optimizer.state_dict()['param_groups'][0]['lr'])\n",
    "\n",
    "        if validate_loss < best_loss:\n",
    "            best_loss = validate_loss\n",
    "            print('Find better model in Epoch {0}, saving model.'.format(epoch))\n",
    "            #torch.save(NET[fold],  model_path+'/best_model_' + str(fold) + '.pt')  # 保存最优模型\n",
    "        #是否满足早停法条件\n",
    "        if(early_stopping(validate_loss,NET[fold])):\n",
    "            print(\"Early stopping\")\n",
    "            break\n",
    "\n",
    "    print('Fold %d Training Finished' %(fold+1))\n",
    "    torch.cuda.empty_cache()# 清空显卡cuda\n",
    "print('Training Finished')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() takes 1 positional argument but 2 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_5716/30499592.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m12\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m5000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTanh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: __init__() takes 1 positional argument but 2 were given"
     ]
    }
   ],
   "source": [
    "test = torch.zeros((1,12,5000))\n",
    "tanh_nn = nn.Tanh()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ------Display the attention value-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# -----For Leads -----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_model_path = \"./model/20220907_091538/best_model_0.pt\"\n",
    "test_model = torch.load(test_model_path).to(DEVICE)\n",
    "test_x = x[0:1698]\n",
    "test_y = x[0:1698]\n",
    "test_x = MAX_MIN_normalization_by_feactures(test_x)\n",
    "test_x = torch.FloatTensor(test_x)  #turn numpy to tensor\n",
    "test_y = torch.LongTensor(test_y)\n",
    "test_dataset = Data.TensorDataset(test_x, test_y)\n",
    "test_dataloader = Data.DataLoader(dataset=test_dataset, batch_size=1, shuffle=False)\n",
    "test_model.eval()\n",
    "\n",
    "test_model.eval()\n",
    "attention_value_timestep = np.zeros(EcgChannles_num,)\n",
    "for i,data in enumerate(test_dataloader,0):\n",
    "    inputs,labels = data[0].to(DEVICE),data[1].to(DEVICE)\n",
    "    outputs = test_model(inputs)\n",
    "    #print(outputs)\n",
    "    _,pred = outputs.max(1) # 求概率最大值对应的标签\n",
    "    #print(\"the label :{labels},pred is {pred}\".format(labels=labels[0],pred=pred[0]))\n",
    "    attention_value = test_model.attention_value\n",
    "    attention_value_timestep += (((attention_value.to('cpu'))[0]).detach().numpy()).sum(axis=0)\n",
    "plt.figure(figsize=(5,5))\n",
    "plt.stem(np.arange(0,len(attention_value_timestep)), attention_value_timestep)\n",
    "plt.xticks(np.arange(0,len(attention_value_timestep)),['I', 'II', 'III', 'aVR', 'aVL', 'aVF', 'V1', 'V2', 'V3', 'V4', 'V5', 'V6'])\n",
    "plt.ylabel(\"Sum of attention value\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(summary(Models.CNN_ATT7(), (EcgChannles_num,EcgLength_num), device=\"cpu\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.13 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
