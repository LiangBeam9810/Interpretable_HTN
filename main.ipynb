{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 1\n",
    "%aimport ecg_get_data\n",
    "%aimport Models\n",
    "%aimport train_test_validat\n",
    "%aimport self_attention\n",
    "%aimport ECGplot\n",
    "%aimport Net\n",
    "%aimport select_dataset\n",
    "import select_dataset\n",
    "import Models \n",
    "import Net\n",
    "from train_test_validat import *\n",
    "from self_attention import *\n",
    "import  ecg_get_data \n",
    "import matplotlib.pyplot as plt\n",
    "import ecg_plot\n",
    "\n",
    "import torch\n",
    "import torch.utils.data as Data\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "import random\n",
    "\n",
    "import time\n",
    "import os\n",
    "\n",
    "# random_seed = 2\n",
    "# torch.manual_seed(random_seed)    # reproducible\n",
    "# torch.cuda.manual_seed_all(random_seed)\n",
    "# random.seed(random_seed)\n",
    "# np.random.seed(random_seed)\n",
    "\n",
    "time_str = time.strftime(\"%Y%m%d_%H%M%S\", time.localtime()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "model_path = './model/'+time_str\n",
    "log_path = './log/'+  time_str\n",
    "ECG_root = '/workspace/data/Preprocess_HTN/data/ECG'\n",
    "EcgChannles_num = 12\n",
    "EcgLength_num = 5000\n",
    "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = select_dataset.splite_dataset('/workspace/data/Preprocess_HTN/data/',True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\n",
      " orginal   fliter department&age\n",
      "  33342       8479   \n",
      "\t\n",
      " orginal    fliterID \n",
      "   509        423    \n",
      "\t\n",
      " orginal    fliterID \n",
      "   8479       7043   \n",
      "lack sample like : Pandas(Index=28, num=2269, name='潘茂元', years=102, gender='男', department='干部保健二科一区', diagnose='高血压3级', ID='354834', date='2021-01-22 16:19:10', ECG_path='21-1-2269_HTN.npy')\n",
      "\t\n",
      "       HTN  NHTN \n",
      "test   423   422 \n",
      "npys:{%d} 845\n",
      "\t\n",
      " orginal   fliter department&age\n",
      "  65933      11329   \n",
      "\t\n",
      " orginal    fliterID \n",
      "   1354       1111   \n",
      "\t\n",
      " orginal    fliterID \n",
      "  11329       9215   \n",
      "lack sample like : Pandas(Index=902, num=22362, name='潘世明', years=97, gender='男', department='干部保健一科一区', diagnose='高血压病3级（极高危）', ID='807057', date='2020-04-15 04:21:14', ECG_path='20-22362_HTN.npy')\n",
      "lack sample like : Pandas(Index=1199, num=3989, name='叶钱', years=93, gender='男', department='干部保健一科一区', diagnose='高血压3级', ID='272796', date='2020-07-07 15:47:48', ECG_path='20-3989_HTN.npy')\n",
      "lack sample like : Pandas(Index=185, num=269, name='陈振羽', years=91, gender='男', department='', diagnose='', ID='', date='', ECG_path='00-269_HTN.npy')\n",
      "\t\n",
      "       HTN  NHTN \n",
      "train  999   999 \n",
      "valid  112   109 \n",
      " add    0   8107 \n",
      "npys:{%d} 221\n",
      "npys:{%d} 1998\n"
     ]
    }
   ],
   "source": [
    "test_list = data.__get_test_file_list__(True)\n",
    "test_Dataset = ecg_get_data.ECG_Dataset(ECG_root,test_list,EcgChannles_num,EcgLength_num)  # type: ignore\n",
    "valid_list,train_list,addition_train_list = data.__get_VT_file_list__(0.9,True)\n",
    "valid_Dataset = ecg_get_data.ECG_Dataset(ECG_root,valid_list,EcgChannles_num,EcgLength_num)\n",
    "train_Dataset = ecg_get_data.ECG_Dataset(ECG_root,train_list,EcgChannles_num,EcgLength_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 0.])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ECG,label = test_Dataset.__getitem__(7)\n",
    "#inf,path = train_Dataset.get_basic_inf(55)\n",
    "#ecg_plot.plot(ECG*3500/1000, sample_rate = 500, title = \"test\",row_height= 10,show_grid=True,show_separate_line=True)\n",
    "label\n",
    "#ecg_plot.save_as_png(inf[1],'/workspace/data/OneDrive - mail.hfut.edu.cn/ECG/Interpretable_HTN//PNG_ECG/',dpi = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128\n",
    "\n",
    "FOLDS = 1\n",
    "EPOCHS = 5000  \n",
    "PATIENCE = 5\n",
    "LR = 0.01\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()# 清空显卡cuda\n",
    "# NET = [\n",
    "#     Net.channels_branch_CNN(True,res = True,se = True,Dropout_rate = 0.25)\n",
    "# ]\n",
    "# NET = [Net.channels_branch_CNN(True,res = True,se = True,Dropout_rate = 0.25) ] # type: ignore\n",
    "NET = [Net.MLBFNet(True,res = True,se = True,Dropout_rate = 0.3) ] # type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "os.makedirs(model_path, exist_ok=True)\n",
    "writer = SummaryWriter(log_path)\n",
    "# writer.add_graph(NET[0], torch.zeros((1,12,5000)))  #模型及模型输入数据\n",
    "torch.cuda.empty_cache()# 清空显卡cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "def linear_combination(x, y, epsilon): \n",
    "    return epsilon*x + (1-epsilon)*y\n",
    "def reduce_loss(loss, reduction='mean'):\n",
    "    return loss.mean() if reduction=='mean' else loss.sum() if reduction=='sum' else loss\n",
    "\n",
    "\n",
    "class LabelSmoothingCrossEntropy(nn.Module):\n",
    "    def __init__(self, epsilon:float=0.1, reduction='mean'):\n",
    "        super().__init__()\n",
    "        self.epsilon = epsilon\n",
    "        self.reduction = reduction\n",
    "    \n",
    "    def forward(self, preds, target):\n",
    "        n = preds.size()[-1]\n",
    "        log_preds = F.log_softmax(preds, dim=-1)\n",
    "        loss = reduce_loss(-log_preds.sum(dim=-1), self.reduction)\n",
    "        nll = F.nll_loss(log_preds, target, reduction=self.reduction)\n",
    "        return linear_combination(loss/n, nll, self.epsilon)\n",
    "    \n",
    "class GCELoss(nn.Module):\n",
    "    def __init__(self, num_classes=2, q=0.7):\n",
    "        super(GCELoss, self).__init__()\n",
    "        self.q = q\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "    def forward(self, pred, labels):\n",
    "        pred = torch.nn.functional.softmax(pred, dim=1)\n",
    "        pred = torch.clamp(pred, min=1e-4, max=1.0)\n",
    "        label_one_hot = torch.nn.functional.one_hot(labels, self.num_classes).float().to(pred.device)\n",
    "        loss = (1. - torch.pow(torch.sum(label_one_hot * pred, dim=1), self.q)) / self.q\n",
    "        return loss.mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\n",
      " orginal   fliter department&age\n",
      "  65933      11329   \n",
      "\t\n",
      " orginal    fliterID \n",
      "   1354       1111   \n",
      "\t\n",
      " orginal    fliterID \n",
      "  11329       9215   \n",
      "lack sample like : Pandas(Index=1068, num=3257, name='史元昌', years=98, gender='男', department='干部保健二科三区', diagnose='高血压', ID='271758', date='2020-11-20 16:04:58', ECG_path='20-3257_HTN.npy')\n",
      "lack sample like : Pandas(Index=1199, num=3989, name='叶钱', years=93, gender='男', department='干部保健一科一区', diagnose='高血压3级', ID='272796', date='2020-07-07 15:47:48', ECG_path='20-3989_HTN.npy')\n",
      "\t\n",
      "       HTN  NHTN \n",
      "train  999   999 \n",
      "valid  112   110 \n",
      " add    0   8106 \n",
      "npys:{%d} 222\n",
      "npys:{%d} 1998\n",
      "- Epoch: 1 - Train_loss: 0.69575 - Train_acc: 0.49324 - F1 score: 0.40664 - Val_loss: 0.69197 - Val_acc: 0.48479 - F1 score: 0.33133 - T_Time: 41.25931\n",
      "当前学习率：0.00000010\n",
      "train:\n",
      " [[880 119]\n",
      " [890 109]]\n",
      "validate:\n",
      " [[110   0]\n",
      " [112   0]]\n",
      "test:\n",
      " [[422   0]\n",
      " [423   0]]\n",
      "Validation F1 score  increase to (inf --> -0.69196689).  Saving model ...\n",
      "                    --------------------------------------------------\n",
      "\n",
      "- Epoch: 2 - Train_loss: 0.69211 - Train_acc: 0.48930 - F1 score: 0.40608 - Val_loss: 0.69023 - Val_acc: 0.49044 - F1 score: 0.33133 - T_Time: 38.35355\n",
      "当前学习率：0.00000010\n",
      "train:\n",
      " [[865 134]\n",
      " [885 114]]\n",
      "validate:\n",
      " [[110   0]\n",
      " [112   0]]\n",
      "test:\n",
      " [[421   1]\n",
      " [423   0]]\n",
      "Validation F1 score  increase to (-0.69196689 --> -0.69023147).  Saving model ...\n",
      "                    --------------------------------------------------\n",
      "\n",
      "- Epoch: 3 - Train_loss: 0.69369 - Train_acc: 0.50826 - F1 score: 0.42471 - Val_loss: 0.69009 - Val_acc: 0.49609 - F1 score: 0.33133 - T_Time: 37.70787\n",
      "当前学习率：0.00000010\n",
      "train:\n",
      " [[886 113]\n",
      " [871 128]]\n",
      "validate:\n",
      " [[110   0]\n",
      " [112   0]]\n",
      "test:\n",
      " [[422   0]\n",
      " [422   1]]\n",
      "Validation F1 score  increase to (-0.69023147 --> -0.69009477).  Saving model ...\n",
      "                    --------------------------------------------------\n",
      "\n",
      "- Epoch: 4 - Train_loss: 0.69449 - Train_acc: 0.48814 - F1 score: 0.39596 - Val_loss: 0.69047 - Val_acc: 0.49609 - F1 score: 0.33133 - T_Time: 37.90297\n",
      "当前学习率：0.00000010\n",
      "train:\n",
      " [[879 120]\n",
      " [902  97]]\n",
      "validate:\n",
      " [[110   0]\n",
      " [112   0]]\n",
      "test:\n",
      " [[422   0]\n",
      " [423   0]]\n",
      "EarlyStopping counter: 1 out of 5\n",
      "\n",
      "- Epoch: 5 - Train_loss: 0.68925 - Train_acc: 0.50244 - F1 score: 0.41572 - Val_loss: 0.69069 - Val_acc: 0.49186 - F1 score: 0.33133 - T_Time: 37.78312\n",
      "当前学习率：0.00000010\n",
      "train:\n",
      " [[887 112]\n",
      " [882 117]]\n",
      "validate:\n",
      " [[110   0]\n",
      " [112   0]]\n",
      "test:\n",
      " [[422   0]\n",
      " [423   0]]\n",
      "EarlyStopping counter: 2 out of 5\n",
      "\n",
      "- Epoch: 6 - Train_loss: 0.68783 - Train_acc: 0.50928 - F1 score: 0.42551 - Val_loss: 0.68947 - Val_acc: 0.50175 - F1 score: 0.33133 - T_Time: 38.22821\n",
      "当前学习率：0.00000010\n",
      "train:\n",
      " [[891 108]\n",
      " [872 127]]\n",
      "validate:\n",
      " [[110   0]\n",
      " [112   0]]\n",
      "test:\n",
      " [[421   1]\n",
      " [423   0]]\n",
      "Validation F1 score  increase to (-0.69009477 --> -0.68946609).  Saving model ...\n",
      "                    --------------------------------------------------\n",
      "\n",
      "- Epoch: 7 - Train_loss: 0.69207 - Train_acc: 0.49170 - F1 score: 0.40912 - Val_loss: 0.69107 - Val_acc: 0.49360 - F1 score: 0.32931 - T_Time: 38.58850\n",
      "当前学习率：0.00000010\n",
      "train:\n",
      " [[864 135]\n",
      " [881 118]]\n",
      "validate:\n",
      " [[109   1]\n",
      " [112   0]]\n",
      "test:\n",
      " [[421   1]\n",
      " [423   0]]\n",
      "EarlyStopping counter: 1 out of 5\n",
      "\n",
      "- Epoch: 8 - Train_loss: 0.69118 - Train_acc: 0.50377 - F1 score: 0.42120 - Val_loss: 0.69055 - Val_acc: 0.49501 - F1 score: 0.32931 - T_Time: 38.32128\n",
      "当前学习率：0.00000010\n",
      "train:\n",
      " [[883 116]\n",
      " [874 125]]\n",
      "validate:\n",
      " [[109   1]\n",
      " [112   0]]\n",
      "test:\n",
      " [[421   1]\n",
      " [423   0]]\n",
      "EarlyStopping counter: 2 out of 5\n",
      "\n",
      "- Epoch: 9 - Train_loss: 0.69496 - Train_acc: 0.48504 - F1 score: 0.39488 - Val_loss: 0.69453 - Val_acc: 0.47557 - F1 score: 0.32727 - T_Time: 38.99322\n",
      "当前学习率：0.00000010\n",
      "train:\n",
      " [[870 129]\n",
      " [900  99]]\n",
      "validate:\n",
      " [[108   2]\n",
      " [112   0]]\n",
      "test:\n",
      " [[421   1]\n",
      " [423   0]]\n",
      "EarlyStopping counter: 3 out of 5\n",
      "\n",
      "- Epoch: 10 - Train_loss: 0.69439 - Train_acc: 0.49735 - F1 score: 0.41152 - Val_loss: 0.69377 - Val_acc: 0.48371 - F1 score: 0.32931 - T_Time: 37.95404\n",
      "当前学习率：0.00000010\n",
      "train:\n",
      " [[874 125]\n",
      " [882 117]]\n",
      "validate:\n",
      " [[109   1]\n",
      " [112   0]]\n",
      "test:\n",
      " [[421   1]\n",
      " [423   0]]\n",
      "EarlyStopping counter: 4 out of 5\n",
      "\n",
      "- Epoch: 11 - Train_loss: 0.69487 - Train_acc: 0.53387 - F1 score: 0.52175 - Val_loss: 0.73524 - Val_acc: 0.49468 - F1 score: 0.33907 - T_Time: 37.91270\n",
      "当前学习率：0.00100000\n",
      "train:\n",
      " [[701 298]\n",
      " [631 368]]\n",
      "validate:\n",
      " [[109   1]\n",
      " [111   1]]\n",
      "test:\n",
      " [[422   0]\n",
      " [420   3]]\n",
      "EarlyStopping counter: 5 out of 5\n",
      "\n",
      "Early stopping\n",
      "Fold 1 Training Finished\n",
      "Training Finished\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "for fold in range(FOLDS):\n",
    "    #每个人fold都重新抽取\n",
    "    NET[fold].to(DEVICE)\n",
    "    valid_list,train_list,addition_train_list = data.__get_VT_file_list__(0.9,True)\n",
    "    valid_Dataset = ecg_get_data.ECG_Dataset(ECG_root,valid_list,EcgChannles_num,EcgLength_num)\n",
    "    train_Dataset = ecg_get_data.ECG_Dataset(ECG_root,train_list,EcgChannles_num,EcgLength_num)\n",
    "    \n",
    "    train_dataloader = Data.DataLoader(dataset=train_Dataset, batch_size=BATCH_SIZE, shuffle=True,num_workers=0,pin_memory=True)\n",
    "    valid_dataloader = Data.DataLoader(dataset=valid_Dataset, batch_size=BATCH_SIZE, shuffle=True,num_workers=0,pin_memory=True)\n",
    "    test_dataloader = Data.DataLoader(dataset=test_Dataset, batch_size=BATCH_SIZE, shuffle=True,num_workers=0,pin_memory=True)\n",
    "    early_stopping = EarlyStopping(PATIENCE, verbose=True, model_path=model_path, delta=0, positive=False)\n",
    "    optimizer  = torch.optim.Adam(NET[fold].parameters(), lr=LR,weight_decay=1e-2)  \n",
    "    warm_up_iter = 10\n",
    "    T_max = 500\t# 周期\n",
    "    lr_max = 1e-3\t# 最大值\n",
    "    lr_min = 1e-5\t# 最小值\n",
    "    lambda0 = lambda cur_iter: lr_min if  cur_iter < warm_up_iter else \\\n",
    "        (lr_min + 0.5*(lr_max-lr_min)*(1.0+math.cos( (cur_iter-warm_up_iter)/(T_max-warm_up_iter)*math.pi)))/0.01\n",
    "    scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=lambda0)\n",
    "    criterion = torch.nn.CrossEntropyLoss()   \n",
    "    # criterion = LabelSmoothingCrossEntropy()\n",
    "    best_test_F1 = 0\n",
    "    for epoch in range(1,EPOCHS):\n",
    "        time_all=0\n",
    "        start_time = time.time()\n",
    "        y_true,y_pred,train_loss,train_acc = train_model(train_dataloader, NET[fold], criterion, optimizer,DEVICE) # type: ignore # 训练模型\n",
    "        time_all = time.time()-start_time\n",
    "        F1_score_train =f1_score(y_true, y_pred, average='macro')#F1分数\n",
    "        C0 = confusion_matrix(y_true,y_pred)\n",
    "        y_true,y_pred,validate_loss,validate_acc = eval_model(valid_dataloader,criterion,NET[fold],DEVICE) # 验证模型\n",
    "        F1_score_valid =f1_score(y_true, y_pred, average='macro')#F1分数\n",
    "        C1 = confusion_matrix(y_true,y_pred)\n",
    "        y_true,y_pred,test_loss,test_acc = eval_model(test_dataloader,criterion,NET[fold],DEVICE) # 验证模型\n",
    "        F1_score_test =f1_score(y_true, y_pred, average='macro')#F1分数\n",
    "        C2 = confusion_matrix(y_true,y_pred)\n",
    "\n",
    "        # writer.add_scalars(main_tag=str(fold)+'_Loss',tag_scalar_dict={'train': train_loss,'validate': validate_loss},global_step=epoch)\n",
    "        # writer.add_scalars(main_tag=str(fold)+'_Accuracy',tag_scalar_dict={'train': train_acc,'validate': validate_acc},global_step=epoch)\n",
    "        # writer.add_scalars(main_tag=str(fold)+'_LearningRate',tag_scalar_dict={'LR': optimizer.state_dict()['param_groups'][0]['lr']},global_step=epoch)\n",
    "        # writer.add_scalars(main_tag=str(fold)+'_F1_score',tag_scalar_dict={'train':F1_score_train,'validate': F1_score_valid},global_step=epoch)\n",
    "        writer.add_scalars(main_tag=str(fold)+'_Loss',tag_scalar_dict={'train': train_loss,'validate': validate_loss,'test':test_loss},global_step=epoch)\n",
    "        writer.add_scalars(main_tag=str(fold)+'_Accuracy',tag_scalar_dict={'train': train_acc,'validate': validate_acc,'test':test_acc},global_step=epoch)\n",
    "        writer.add_scalars(main_tag=str(fold)+'_LearningRate',tag_scalar_dict={'LR': optimizer.state_dict()['param_groups'][0]['lr']},global_step=epoch)\n",
    "        writer.add_scalars(main_tag=str(fold)+'_F1_score',tag_scalar_dict={'train':F1_score_train,'validate': F1_score_valid,'test':F1_score_test},global_step=epoch)        \n",
    "        print('- Epoch: %d - Train_loss: %.5f - Train_acc: %.5f - F1 score: %.5f - Val_loss: %.5f - Val_acc: %.5f - F1 score: %.5f - T_Time: %.5f' %(epoch,train_loss,train_acc,F1_score_train,validate_loss,validate_acc,F1_score_valid,time_all))\n",
    "        print('当前学习率：%.8f' %optimizer.state_dict()['param_groups'][0]['lr'])\n",
    "        print('train:\\n',C0)\n",
    "        print('validate:\\n',C1)\n",
    "        print('test:\\n',C2)\n",
    "        \n",
    "        if(F1_score_test>best_test_F1):\n",
    "            best_test_F1 = F1_score_test\n",
    "            torch.save(NET[fold].state_dict(), model_path+'/parameter_best_test_' + str(fold) + '.pt')\n",
    "        \n",
    "        scheduler.step() # 学习率迭代\n",
    "        #是否满足早停法条件\n",
    "        if(early_stopping(validate_loss,NET[fold],fold)):\n",
    "            print(\"Early stopping\")\n",
    "            break\n",
    "\n",
    "    print('Fold %d Training Finished' %(fold+1))\n",
    "    torch.cuda.empty_cache()# 清空显卡cuda\n",
    "print('Training Finished')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.13 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
